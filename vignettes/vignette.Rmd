---
title: "kDGLM: an R package for Bayesian analysis of Dynamic Generialized Linear Models"
author: Silvaneo V. dos Santos Jr.
output: rmdformats::readthedown
date: "`r Sys.setlocale('LC_TIME', 'English'); format(Sys.Date(),'%d of %B, %Y')`"
bibliography: '`r system.file("REFERENCES.bib", package="kDGLM")`'
link-citations: TRUE
urlcolor: blue
linkcolor: green
vignette: >
  %\VignetteIndexEntry{kDGLM: an R package for Bayesian analysis of Dynamic Generialized Linear Models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = ""
)

# library(kDGLM)
devtools::load_all()
```

# Introduction

This vignette is intended as an introduction to use of the kDGLM package, which offers routines for Bayesian analysis of Dynamic Generialized Linear Models, incluinding fitting (filtering and smoothing), forecasting, sampling, intervation and automated monitoring, following the theory developed and/or explored in @Kalman_filter_origins, @WestHarr-DLM and @ArtigokParametrico.

In this document we will focus exclusively in the usage of the package and will only briefly mention the theory behind these models and only with the intention of highlighting the notation. We highly recommend all users to read the theoretical work in which we based this package.

This document is organized in the following order: 

1. First we introduce the notations and the class of models we will be dealing with;
2. Then we present some basic examples of model fitting, also showing the auxiliary functions that help the user to analyse the fitted model;
3. Next we present the details about the specification of the model structure, offering tools that allow for an easy, fast and (hopefully) intuitive way of defining the model;
4. In the following section we discuss details about how the user can specify the observational model;
5. In the next section we present a variety of more advanced examples, combining the basic features shown in previous sections to create more complex models. We also show tools for easy model selection;
6. In this section, we show how the user can combine different types of outcomes in a same model to analyse multiple time series simultaneously;
7. Lastly, we show how to make interventions on the model and also present the tools offered for automatic monitoring of time series.

# Notation

Let us assume that the user is interested in analyzing a Time Series $\{Y_t\}_{t=1}^T$ which can be described by the following model:

$$
\begin{align}
Y_t|\eta_t &\sim \mathcal{F}\left(\eta_t\right),\\
g(\eta_t) &=\lambda_{t}=F_t'\theta_t,\\
\theta_t&=G_t\theta_{t-1}+\omega_t,\\
\omega_t &\sim \mathcal{N}(h_t,W_t),
\end{align}
$$
where:


- $\theta_t$ are the unknown parameter of interest;
- $\mathcal{F}$ is a probability distribution in the Exponential Family and indexed by $\eta_t$;
- $g$, called the link function, is a pre-specified function (in this package, each choice of $\mathcal{F}$ will have a pre-specified $g$);
- $F_t$, called the design matrix, is a (mostly) known matrix specified by the user;
- $G_t$, called the evolution matrix, is a (mostly) known matrix specified by the user;
- $h_t$ is a known vector specified by the user. In general, we have $h_t=0$, except when making interventions on the model [see @WestHarr-DLM, chapter 11];
- $W_t$ is a known covariance matrix specified by the user.

Also, following the notation in @WestHarr-DLM, we will define $\mathcal{D}_t$ as the information one has after the first $t$ observations, such that $\mathcal{D}_t=\mathcal{D}_{t-1}\cup\{Y_t\}$ (for now, let's assume that there is no external source of information beside $Y_t$ itself) and $\mathcal{D}_0$ denotes the information we have about the process $\{Y_t\}^T_{t=1}$ *prior* to observing the data.

For the specification of $W_t$, we follow the ideas presented @WestHarr-DLM, section 6.3, such that we define $W_t=Var[G_t\theta_{t-1}|\mathcal{D}_{t-1}] \odot (1-D_t) \oslash D_t + H_t$, where $D_t$ (called the discount matrix) is a matrix of values between $0$ and $1$ (including $1$, but excluding $0$), $\odot$ is the Hadamard product (i.e., element-wise product) , $\oslash$ is the Hadamard division (i.e., element-wise division) and $H_t$ is a known matrix specified by the user. Notice that this specification implies that $Var[\theta_{t}|\mathcal{D}_{t-1}]=Var[G_t\theta_{t-1}|\mathcal{D}_{t-1}] \oslash D_t + H_t$, such that, if all entries of $D_t$ are equal to $1$, then $Var[\theta_{t}|\mathcal{D}_{t-1}]=Var[G_t\theta_{t-1}|\mathcal{D}_{t-1}]+H_t$. If also all entries of $H_t$ are equal to 0, then $Var[\theta_{t}|\mathcal{D}_{t-1}]=Var[G_t\theta_{t-1}|\mathcal{D}_{t-1}]$ (i.e. $\theta_t$ is a deterministic transformation of $\theta_{t-1}$), such that our model becomes equivalent to a Generalized Linear Model.

Notice that we **did not** make any assumptions on the dimension of $Y_t$ or $\eta_t$, indeed, this package offers tools for the fitting time series of vectors (as in the Multinomial case) and/or with multiple unknown parameters (as in the Normal with unknown variance case).

Naturally, the manual specification of all the structural components of the model can be quite tiresome. With that in mind, our package offers a wide and ever expanding set of auxiliary functions that aim to help the user to specify the struture of a model. In the next section we will explore some of those tools.

# Fitting and analysing models

Let's start with the central function offered by this package, the `fit_model` function. In this section we present the basic usage of this function. We also present some of the tools for graphical analysis of fitted models.

## Filtering, smoothing and sampling
 
The usage of the `fit_model` function is as follows:

```{r echo=TRUE, eval=FALSE}
fit_model(
  ...,
  outcomes,
  pred_cred = 0.95,
  lag = -1,
  p_monit = NA,
  c_monit = 1
)
```

The `outcomes` argument must be a list of `dlm_distr` objects, which will will describe in detail in a later section. `pred_cred` is a numeric value representing the desired credibility of the predictions made while fitting (if `pred_cred` is not a valid value, such as `NA`, no predictions are made). `smooth` is a boolean indicating if the smoothed distribution of the latent variables should be calculated (generally we recommend the users to not change this value, as the computational cost of smoothing is almost always negligible). `p_monit` and `c_monit` control the sensibility of the automated monitoring and we shall discuss their usage in a later section. Lastly, all other arguments passed are assumed to be `dlm_block` objects (whose creation we will detail in a later section) defining the model structure.

As an usage example, let's assume that we want to fit the following model:

$$
\begin{align}
Y_t|\eta_t &\sim Poisson\left(\eta_t\right),\\
\ln(\eta_t) &=\lambda_{t}=\mu_t+\theta_{1t},\\
\mu_t&=\mu_{t-1}+\omega_t,\\
\begin{bmatrix}\theta_{1t}\\\theta_{2t}\end{bmatrix}&=J_2(1,2\pi/12)\begin{bmatrix}\theta_{1t-1}\\\theta_{2t-1}\end{bmatrix},\\
\omega_t &\sim \mathcal{N}(0,0.02),
\end{align}
$$
where $J_2(1,2\pi/12)$ is a rotation matrix with phase $2\pi/12$, such that we have seasonality with period $12$ [see @WestHarr-DLM, section 8.6, for details].

In the next section we will discuss the details of the specification of the model structure, for now, we will only present how to fit such model, which can be done with the following code:

```{r include=FALSE}
# Poisson case
T <- 120
data <- rpois(T, exp(
  5 + cumsum(rnorm(T, 0, sqrt(0.002))) +
    0.1 * cos(2 * pi * (1:T) / 12)
))
```


```{r}
# Structure
level <- polynomial_block(rate = 1, H = 0.002)
season <- harmonic_block(rate = 1, period = 12)

# Outcome
outcome <- Poisson(lambda = "rate", outcome = data)

# Fitting and ploting
fitted_data <- fit_model(level, season, outcomes = outcome)
```


The first two lines creates structural blocks representing a random walk on $\mu_t$ and a seasonal component described by harmonics. The third line creates a Poisson outcome such that the rate parameter lambda is equal to $\exp\{\text{rate}\}$, since that was the name given to the linear predictor when creating the structural blocks (details about the linear predictor will be given in the next section). The last line receives the model structure and the Poisson outcome and fits the model, obtaining the parameter for the filtered and smoothed distribution of $\mu_t$, $\theta_{1t}$ and $\theta_{2t}$.

The user may see how to model fits the data using the `plot` method, which is a wrapper for the `show_fit` function:

```{r}
# show_fit(fitted_data)
# or
plot(fitted_data, plot_pkg = "base")
```

As default, this method return the one-step ahead prediction for the observations, but the user may change the arguments of this function such as to evaluate the $h$-steps ahead or the smoothed predictions. See the documentation of the `show_fit` function for more details.

To see a summary of the fitted model, one can use the `report_dlm` function (which is wrapped in the `summary` method):

```{r}
# report_dlm(fitted_data)
# or
summary(fitted_data)
```

Notice that the coefficients shown are those of the last observation (time 200), but one can also see the summary of any particular time by changing the `t` argument:

```{r}
summary(fitted_data, t = 100)
```

For more details about the usage of the `report_dlm` function, see the associated documentation.

Naturally, the user might want to access the estimated values of each parameter, along with the parameters for they posterior distribution. The `coef` method, which is a wrapper for the `eval_past` function, can provide this information for the user:

```{r echo=TRUE, eval=FALSE}
# eval_past(fitted_data, lag=-1)
# or
coef(fitted_data, lag = -1)
```

The output of this function is a list containing:

-``data``: A data frame with the model evaluated at each observed time.

-``mt``: A $n \times T$ matrix representing the mean of the latent variables at each time, where $n$ is the number of latent variables in the model and $T$ is the length of the time series.

-``Ct``: A 3D-array containing the covariance matrix of the latent variable at each time. Dimensions are $n \times n \times T$.

-``ft``: A $k \times T$ matrix representing mean of the linear predictor at each time, where $k$ is the number of linear predictors in the model and $T$ is the length of the time series.

-``Qt``: A 3D-array containing the covariance matrix for the linear predictor at each time. Dimensions are $k \times k \times T$.

The user may also want to plot the latent variables, for which he (?) can use the `plot_lat_var` function:

```{r}
plot_lat_var(fitted_data, plot_pkg = "base")$plot
```

The user may also plot the linear predictors, for that he can use the `plot_lin_pred` function:

```{r}
plot_lin_pred(fitted_data, plot_pkg = "base")$plot
```

Notice that, in all cases, estimation is restricted to the period where the model was fitted. If the user wishes make predictions for future observations, he (?) can use the `forecast` function:

```{r}
forecast(fitted_data, 20, plot = "base")$plot
```


For details on the usage of this function, see the associated documentation.

Lastly, one may also want to draw samples from the latent variables, linear predictors and/or the parameters $\eta_t$. For that, our package offers the `dlm_sampling`, which provides a routine for drawing independent samples from the posterior of all parameters with low computational cost [see @WestHarr-DLM, chapter 15, for details]:


```{r eval=FALSE}
dlm_sampling(fitted_data, 5000)
```

# Creating the model structure

In this sections we will discuss the specification of the model structure. We will consider the structure of a model as all the elements that determine the relation between our linear predictor $\lambda_t$ and our latent states $\theta_t$ though time, namely, the structure of a model consists of the matrices $F_t$, $G_t$, $h_t$, $H_t$ and $D_t$. Although we allow the user to manually define each entry of each of these matrices, we also offer tools to simplify this task. Let's start by presenting the basis function for all structural blocks, the `polynomial_block` function.

## A structure for polynomial trend models

```{r eval=FALSE, include=TRUE}
polynomial_block(
  ...,
  order = 1,
  name = "Var_Poly",
  D = 1,
  h = 0,
  H = 0,
  m0 = 0,
  C0 = 1
)
```

This function will create a latent vector $\theta_t=(\theta_{1t},...,\theta_{k\ t})'$,, where $k$ is the order of the polynomial block, so that:

$$
\begin{align}
\theta_{it} &= \theta_{it-1}+\theta_{i+1\ t-1}+\omega_{it}, i=1,...,k-1\\
\theta_{kt} &= \theta_{kt-1}+\omega_{kt},\\
\theta_1&\sim \mathcal{N}_k(m_0,C_0),\\
\omega_{1t},...,\omega_{kt}&\sim \mathcal{N}_k(h_t,W_t),
\end{align}
$$
where $W_t=Var[\theta_t|\mathcal{D}_{t-1}]\odot (1-D_t) \oslash D_t+H_t$.

Notice that the user do not need to specify the matrix $G_t$, since it is implicitly determined by the order of the polynomial block (each type of block will define it own matrix $G_t$).

It is easy to see the correspondence between most of the arguments of the `polynomial_block` function and their respective meaning in the block specification, remaining only to explain the use of the `...` and `name` arguments.

The argument `...` is used to specify the matrix $F_t$, specifically, the user must provide a list of named values, whose name indicate a linear predictor $\lambda_t$ and it's associated value represent the effect of $\theta_{1t}$ in this predictor (the other latent states are assumed to have no effect on the linear predictors). Naturally, if a linear predictor is not present in `...`, it is understood that $\theta_{1t}$ have no effect whatsoever on that particular linear predictor. Such specification of $F_t$ may seem strange, but as the reader will see further bellow, this way of specifying $F_t$ is very useful to avoid confusion when dealing with multiple linear predictors.

It is important to emphasize that $\theta_t$ is implicitly determined by the block structure, besides, $\lambda_t$ is implicitly determined by `...`, since one linear predictor will be created for each named value passed by the user.

Lastly, the argument name is optional and it is used to help the user to identify each latent state after fitting the model, specifically, each latent state will be called by it's own name when plotting or printing $\theta_t$.

Notice that the default values of each argument leads to a first order polynomial block with no temporal dynamic, besides, the default prior variance of the first parameter is not specified. Indeed, if the user does not specify the prior, we assume that $\theta_{1i} \sim \mathcal{N}(0,1), i=2,...,k$.

To exemplify the usage of this function, let's assume that we have a simply Normal model with known variance $\sigma^2$, in which $\eta$ is the mean parameter and the link function $g$ is such that $g(\eta)=\eta$. Let's also assume that the mean is constant over time and we have no explanatory variables, such that our model can be written simple as:

$$
\begin{align}
Y_t|\theta_t &\sim \mathcal{N}\left(\eta_t, \sigma^2\right),\\
\eta_t &=\lambda_{t}=\theta_t,\\
\theta_t&=\theta_{t-1}=\theta.
\end{align}
$$

In this case, we have that $F_t=1$, $G_t=1$, $D_t=1$, $h_t=0$ and $H_t=0$, for all $t$. Assuming a prior distribution $\mathcal{N}(0,1)$ for $\theta$, we can create such structure using the following code:

```{r eval=FALSE, include=TRUE}
mean_block <- polynomial_block(
  eta = 1,
  order = 1,
  name = "Mean",
)
```

By setting `eta=1`, we specify that there is a linear predictor called *eta*, and that $eta = 1 \times \theta$. By setting `order = 1`, we specify that $\theta_t$ is a scalar and that $G_t=1$. We can omit the values of `m0` , `C0`, `D`, `h` and `H`, since the default values are equal to the desired. We could also omit the argument `order`, since the default is already $1$, but we chose to explicit define it so as to emphasize it's usage.

Suppose now that we have a explanatory variable $X$ that we would like to introduce in our model to help explain the behavior of $\eta_t$. We could similarly define such structure by creating an additional block such as:

```{r eval=FALSE, include=TRUE}
polynomial_block(
  eta = X,
  name = "Var X"
)
```

By setting `eta=X`, we specify that there is a linear predictor called *eta*, and that $eta = X \times \theta$. If $X=(X_1,...,X_T)'$ is a vector, then we would have $F_t=X_t$, for each $t$, such that $\eta_t = X_t \times \theta_t$.

Until now, we only discussed the creation of static models, but the inclusion of temporal dynamic is very straightforward, one must simply specify the values of `H` to be greater than 0 and/or the values of `D` to be lesser than 1:

```{r eval=FALSE, include=TRUE}
mean_block <- polynomial_block(
  eta = 1,
  order = 1,
  name = "Mean",
  D = 0.95
)
```

Bellow we present a plot of two models fitted to the same data: one with a static mean and another using a dynamic mean.

```{r echo=FALSE}
# Normal case
T <- 200
mu <- rnorm(T, 0, 0.1)
data <- rnorm(T, cumsum(mu))

level1 <- polynomial_block(
  mu1 = 1,
  D = 1,
  name = "Static mean"
)
level2 <- polynomial_block(
  mu2 = 1,
  D = 0.95,
  name = "Dynamic mean"
)
# Known variance
outcome <- list(
  "Static mean" = Normal(mu = "mu1", Sigma = 1, outcome = data),
  "Dynamic mean" = Normal(mu = "mu2", Sigma = 1, outcome = data)
)

fitted_data <- fit_model(level1, level2, outcomes = outcome)

plot(fitted_data, lag = -1, plot_pkg = "base")
```

The detailed theory behind the structure discussed in this section can be found in chapters 6, 7 and 9 from @WestHarr-DLM.

## A structure for dynamic regression models

```{r eval=FALSE, include=TRUE}
regression_block(
  ...,
  lag = 0,
  zero_fill = TRUE,
  name = "Var_Reg",
  D = 1,
  h = 0,
  H = 0,
  m0 = 0,
  C0 = diag(lag + 1)
)
```

The `regression_block` function creates a structural block for a dynamic regression for a covariate $X_t$, as specified in @WestHarr-DLM, chapter 9. When `lag` is equal to 0, this function can be see as a wrapper for the `polynomial_block` function with order equal to 1. When `lag` is greater or equal to 1, `regression_block` function is equivalent to the superposition of several `polynomial_block` function with order equal to 1. As such, it's usage is almost identical to that of the `polynomial_block` function (see the associated documentation for detail about the arguments `lag` and `zero_fill`).

Here we present the code for fitting the following model:

$$
\begin{align}
Y_t|\theta_t &\sim Poisson\left(\eta_t\right),\\
\ln(\eta_t) &=\lambda_{t}=X_t\theta_t,\\
\theta_t&=\theta_{t-1}+\omega_t,\\
\omega_t &\sim \mathcal{N}(0,W_t),
\end{align}
$$
where $X_t$ is a known covariate and $W_t$ is specified using a discount factor of $0.95$.

```{r include=FALSE}
T <- 200
X <- rgamma(T, 20, 20 / 5)
sd_gamma <- 0.1 / (2 * sqrt(T))
gamma_coef <- 0.5 + cumsum(rnorm(T, 0, 0.1 / (2 * sqrt(T))))
data <- rpois(T, exp(gamma_coef * X))
```


```{r}
regression <- regression_block(rate = X, D = 0.95)

outcome <- Poisson(lambda = "rate", outcome = data)

fitted_data <- fit_model(regression, outcomes = outcome)
summary(fitted_data)
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
plot_lat_var(fitted_data, plot_pkg = "base", cutoff = 0) # $plot +
# geom_point(aes(x = time, y = values, color = "True values", fill = "True values"), inherit.aes = FALSE, data = data.frame(time = 1:T, values = gamma_coef, color = "True values"), alpha = 0.75) +
# scale_color_manual("",
#   labels = c(
#     "True values" = "True",
#     "Var_Reg \n(cred. 95%)" = "Estimated"
#   ),
#   values = c("Var_Reg \n(cred. 95%)" = rainbow(1, s = 0.5)[1], "True values" = "black")
# ) +
# scale_fill_manual("",
#   labels = c(
#     "True values" = "True",
#     "Var_Reg \n(cred. 95%)" = "Estimated"
#   ),
#   values = c("Var_Reg \n(cred. 95%)" = rainbow(1, s = 0.5)[1], "True values" = "black")
# ) +
# theme_bw()
```

## A structure for harmonic trend models

```{r eval=FALSE, include=TRUE}
harmonic_block(...,
  period,
  name = "Var_Sazo",
  D = 1,
  h = 0,
  H = 0,
  m0 = 0,
  C0 = 1
)
```

This function will create a latent vector $\theta_t=(\theta_{1t},\theta_{2t})'$, so that:

$$
\begin{align}
\begin{bmatrix}\theta_{1t}\\ \theta_{2t}\end{bmatrix} &= \begin{bmatrix}cos(w) & sin(w)\\ -sin(w) & cos(w)\end{bmatrix}\begin{bmatrix}\theta_{1t}\\  \theta_{2t}\end{bmatrix}\\
\theta_1&\sim \mathcal{N}_2(m_0,C_0),\\
\epsilon_{1t},...,\epsilon_{kt}&\sim \mathcal{N}_k(0,W_t),
\end{align}
$$
where $W_t=Var[\theta_t|\mathcal{D}_{t-1}]\odot (1-D_t) \oslash D_t+H_t$ and $w=\frac{2 * \pi}{period}$.

Notice that the user do not need to specify the matrix $G_t$, since it is implicitly determined by the period of the harmonic block, being a rotation matrix for an angle $w$, such that, if `period` is an integer, $G_t^{period}=I$. This way, when `period` is an integer it represents the length of the seasonal cycle. For instance, if we have a time series with monthly observations and we believe this series to have an annual pattern, then we would set the `period` for the harmonic block to be equal to 12 (the number of observations until the cycle "resets").

Similar to the `polynomial_block` function, the argument `...` is used to specify the matrix $F_t$, specifically, the user must provide a list of named values, whose name indicate a linear predictor $\lambda_t$ and it's associated value represent the effect of $\theta_{1t}$ in this predictor (the other latent state is assumed to have no effect on the linear predictors). Naturally, if a linear predictor is not present in `...`, it is understood that $\theta_{1t}$ have no effect whatsoever on that particular linear predictor. This allows the user to specify seasonal trends for the model and explanatory variables with seasonal effect on the linear predictor, both with support for temporal dynamic.

Here we present a simply usage example for a harmonic block with period 12:

```{r eval=FALSE, include=TRUE}
mean_block <- harmonic_block(
  eta = 1,
  period = 12,
  D = 0.95
)
```

Bellow we present a plot of a Poisson model with such structure:

```{r}
# Poisson case
T <- 60
w <- 2 * pi / 12
data <- rpois(T, exp(1.5 * cos(w * 1:T)))
```


```{r echo=FALSE}
season <- harmonic_block(rate = 1, period = 12, D = 1)

outcome <- Poisson(lambda = "rate", outcome = data)

fitted_data <- fit_model(season, outcomes = outcome)

plot(fitted_data, lag = -1, plot_pkg = "base")
```


The detailed theory behind the structure discussed in this section can be found in chapters 6, 8 and 9 from @WestHarr-DLM.

## A structure for auto regresive models

```{r eval=FALSE, include=TRUE}
AR_block(
  ...,
  order,
  noise_var,
  pulse = 0,
  name = "Var_AR",
  AR_support = "free",
  D = 1,
  h = 0,
  H = 0,
  m0 = 0,
  C0 = 1,
  m0_states = 0,
  C0_states = 1,
  D_states = 1,
  h_states = 0,
  m0_pulse = 0,
  C0_pulse = 1,
  D_pulse = 1,
  h_pulse = 0,
  H_pulse = 0
)
```

This function will create a latent state $\theta_t$, an auto regressive (AR) coefficient vector $\phi_t=(\phi_{1t},...,\phi_{kt})'$, and an pulse coefficient vector $\rho_t=(\rho_{1t},...,\rho_{lt})'$, where $k=order$ and $l$ is the number of pulses (discussed later on) so that:

$$
\begin{align}
\theta_{t} &= \sum_{i=1}^{k}\phi_{it}\theta_{t-i}+\sum_{i=1}^{l}\rho_{it}X_{it}+\epsilon_{t},\\
\phi_t&=\phi_{t-1}+\omega_t,\\
\rho_t&=\rho_{t-1}+\omega_t^{pulse},\\
\epsilon_{t}&\sim \mathcal{N}(0,\sigma^2),\\
\omega_{t}&\sim \mathcal{N}_k(0,W_t),\\
\omega_{t}^{pulse}&\sim \mathcal{N}_l(0,W_t^{pulse}),\\
\theta_1&\sim \mathcal{N}(m_0^{states},C_0^{states}),\\
\phi_1&\sim \mathcal{N}_k(m_0,C_0),\\
\rho_1&\sim \mathcal{N}_l(m_0^{pulse},C_0^{pulse}).
\end{align}
$$
where:

$$
\begin{align}
\sigma^2&=noise\_var&+&Var[\theta_t|\mathcal{D}_{t-1}] &\odot& (1-D_t^{states}) &\oslash& D_t^{states},\\
W_t&=H_t&+&Var[\phi_t|\mathcal{D}_{t-1}] &\odot& (1-D_t) &\oslash& D_t,\\ W^{pulse}_t&=H_t^{pulse}&+&Var[\rho_t|\mathcal{D}_{t-1}] &\odot&(1-D_t^{pulse}) &\oslash&D_t^{pulse}&
\end{align}
$$
and $X$, called pulse matrix, is a known $T \times l$ matrix.

Notice that the user do not need to specify the matrix $G_t$, since it is implicitly determined by the order of the AR block and the equation above, although, as the reader might have noticed, that evolution will always be non linear. Since the method used to fit models in this package requires a linear evolution, we use the approach described in @WestHarr-DLM, chapter 13. Also, for more details about the usage of auto regressive models in the context of DLM's, see @WestHarr-DLM, chapter 9.

It is easy to understand the meaning of most arguments of the `AR_block` function based on the equation above, but some explanation is still needed for the `...`, `name` and `AR_support` arguments, plus the arguments related with the so called pulse.

Similar to the `polynomial_block` function, the argument `...` is used to specify the matrix $F_t$, specifically, the user must provide a list of named values, whose name indicate a linear predictor $\lambda_t$ and it's associated value represent the effect of $\theta_{t}$ in this predictor (we assume that $\phi_t$ and $\rho_t$ have no effect on the linear predictors). Naturally, if a linear predictor is not present in `...`, it is understood that $\theta_{t}$ has no effect whatsoever on that linear predictor.

Also, in the same fashion that the other structure functions work, the `name` argument is optional, providing an easy way to identify each latent variable when plotting and summarizing a fitted model.

The `AR_support` is a character string, either `"constrained"` or `"free"`. If `AR_support` is `"constrained"`, then the AR coefficients $\phi_t$ will be forced to be on the interval $(-1,1)$, otherwise, the coefficients will be unrestricted. Beware that, under no restriction on the coefficients, there is no guarantee that the estimated coefficients will imply in a stationary process, furthermore, if the order of the AR block is greater than 1, then the restriction imposed when AR_support is equal to `"constrained"` does **NOT** guarantee that the process will be stationary (although it may help). To constrain $\phi_t$ to the interval $(-1,1)$, we apply the inverse Fisher transformation, also known as the hyperbolic tangent function.

The pulse matrix $X$ is informed through the argument pulse, with the dimension of $\rho_t$ being deduce by the number of columns in $X$. It is important to notice that the package expects that $X$ will inform the pulse value for each time instance, interpreting each column as a distinct pulse with an associated coordinate of $\rho_t$.

Finally, we can summarize the usage of the `AR_block` function as:

- The arguments `m0`, `C0` are the parameter for the prior for the AR coefficient $\phi_1$;
- The arguments `D`, `h` and `H` define the variance of fluctuations of $\phi_t$ through time;
- The arguments `m0_state`, `C0_state` are the parameter for the prior for the latent states $\theta_1, ...\theta_{1-k}$;
- The arguments `noise_var`, `h_state` and `D_state` define the variance of fluctuations of $\theta_t$ through time;
- The arguments `m0_pulse`, `C0_pulse` are the parameter for the prior for the pulse coefficient $\rho_1$;
- The arguments `D_pulse`, `h_pulse` and `H_pulse` define the variance of oscillation of $rho_t$ through time; 
- The argument `pulse` is the pulse matrix $X$;
- The argument `AR_support` defines the support for the AR coefficients $\phi_t$.

Bellow we present the code for a simply $AR(1)$ block with $\sigma^2=0.1, \forall t$: 

```{r eval=FALSE, include=TRUE}
mean_block <- AR_block(
  eta = 1,
  order = 1,
  noise_var = 0.1
)
```

Finally we present a plot of a Gamma model with known shape $\alpha=1.5$ and a AR structure for the mean fitted with simulated data. We will refrain to show the code for fitting the model itself, since we will discuss the tools for fitting in a section of it's own.

```{r echo=FALSE, fig.height=9, fig.width=16}
# Poisson case
set.seed(13031998)
T <- 200
phi <- 0.95
sigma2 <- 0.1
ht <- rep(NA, T)
ht_i <- 0
for (i in 1:T) {
  ht_i <- phi * ht_i + rnorm(1, 0, sqrt(sigma2))
  ht[i] <- ht_i
}
# plot(exp(ht))

data <- rgamma(T, 3 / 2, (3 / 2) / exp(ht))

volatility <- AR_block(
  eta = 1, order = 1,
  D = 1,
  noise_var = sigma2,
  AR_support = "free",
  m0 = 1
)

############################################################
fitted_data <- fit_model(volatility,
  outcomes = Gamma(phi = 3 / 2, mu = "eta", outcome = data, alt_method = FALSE)
)

# show_fit(fitted_data, lag=-1)$plot
par(mfrow = c(1, 2))
x <- seq(fitted_data$mts[2, T] - 4 * sqrt(fitted_data$Cts[2, 2, T]),
  fitted_data$mts[2, T] + 4 * sqrt(fitted_data$Cts[2, 2, T]),
  l = 1000
)
fx <- dnorm(x, fitted_data$mts[2, T], sqrt(fitted_data$Cts[2, 2, T]))
plot(x, fx, main = "Posterior distribuition for the AR coefficient", type = "l", xlab = phi, ylab = "Density")
lines(c(0.95, 0.95), c(0, max(fx) + 1), lty = 2)
legend("topright", legend = c("True phi"), lty = c(2))

plot(ht, main = "Latent states estimation", xlab = "Time", ylab = "theta_t")
lines(fitted_data$mts[1, ])
lines(fitted_data$mts[1, ] - 1.96 * sqrt(fitted_data$Cts[1, 1, ]), lty = 2)
lines(fitted_data$mts[1, ] + 1.96 * sqrt(fitted_data$Cts[1, 1, ]), lty = 2)
legend("bottomleft", legend = c("True states", "Estimated states"), lty = c(0, 1), pch = c(1, NA))
par(mfrow = c(1, 1))
```


## Handling multiple structural blocks

In the previous subsections, we discussed how to define how to define the structure of a model using the functions `polynomial_block`, `harmonic_block` and `AR_block`, yet we have dealt only with cases when a model had only one of those structures. Generally, the user will want to mix and match multiple types of structures, each one being responsible to explain part of the outcome $Y_t$. For this task, we introduce two operator: one to combine (superposition) and other to multiple blocks.

Suppose that we have a model with a variety of structural blocks such that:

$$
\begin{align}
\theta_t&=\begin{bmatrix}\theta_t^1\\ \vdots\\ \theta_t^n\end{bmatrix}\\
F_t&=\begin{bmatrix}F_t^1 & \dots & F_t^n\end{bmatrix}\\
G_t&=diag\{G_t^{1},...,G_t^{n}\},\\
W_t&=diag\{W_t^{1},...,W_t^{n}\},
\end{align}
$$
where $diag\{M^1,...,M^{n}\}$ represents a block diagonal matrix such that it's diagonal is composed of  $M^1,...,M^{n}$, $\theta_t$ is vector obtained by the concatenation of the vectors $\theta_t^1,..., \theta_t^n$ and $F_t$ is a matrix obtained by the column-wise concatenation of the matrices $F_t^1,..., F_t^n$ (remember that each line of $F_t$ correspond to one linear predictor, such that $F_t^1,..., F_t^n$ have all the same number of lines).

In this scenario, to facilitate the specification of such model, we could create one structural block for each $\theta_t^i$, $F_t^{i}$, $G_t^{i}$ and $W_t^{i}$, $i=1,...n$ and than "combine" all blocks together. This operation is called *superposition* and can be found described in details in @WestHarr-DLM, section 6.2. Our package allows that operation through the function `block_superpos` or, (almost always) equivalently, through the `+` operator:

```{r eval=FALSE, include=FALSE}
block_1 <- ...
.
.
.
block_n <- ...

complete_structure <- block_superpos(block_1, ..., block_n)
# or
complete_structure <- block_1 + ... + block_n
```

To futher demostrate the usage of this operator, suppose we would like to create a model using the three structures presented in the previous sections (a polynomial trend, a harmonic trend and a AR model). Then, we could do so with the following code:

```{r eval=FALSE, include=TRUE}
poly_subblock <- polynomial_block(
  eta = 1,
  order = 1,
  name = "Poly",
  D = 0.95
)

harm_subblock <- harmonic_block(
  eta = 1,
  period = 12,
  name = "Harm",
  D = 0.95
)

AR_subblock <- AR_block(
  eta = 1,
  order = 1,
  noise_var = 0.1,
  name = "AR"
)

complete_block <- poly_subblock + harm_subblock + AR_subblock
```

Ideally, the user should provide each block with a name to help identify then after the model is fitted, but, if the user does not provide a name, then the block will have the default name for that type of block. In case one or more blocks have the same name, the package will automatically add an index to the variables with conflicting names based on the order that the blocks were combined, but this might make the analysis of the fitted model confusing, specially when dealing with a large number of latent variable. With that in mind, we **strongly** recommend the users to specify an intuitive name for each structural block.

Lastly, we present the `block_mult` function and it's associated operator `*`. This function allows the user to create multiple blocks with identical structure, but each one being associated with a different linear predictor. The usage of this function is as simple as:

```{r echo=TRUE}
base_block <- polynomial_block(
  eta = 1,
  order = 1,
  name = "Poly",
  D = 0.95
)

final_block <- block_mult(base_block, 4)
# or
final_block <- 4 * base_block
# or
final_block <- base_block * 4
```

When multiplying block, the package understand that each copy of the base block is independent of each other (i.e., they have their own latent states) and each block is associated with a different set of linear predictors. The linear predictor of each block are taken to be the original name with a index:

```{r echo=TRUE}
final_block <- 4 * base_block
final_block$pred_names
```


Naturally, the user might want to rename the linear predictor to a more intuitive label. For such task, we provide the function `rename_block`, whose usage is as follows:

```{r echo=TRUE}
final_block <- block_rename(final_block, c("Matthew", "Mark", "Luke", "John"))
final_block$pred_names
```

# Creating the model outcome

Until now, we have presented the tools for creating the structure of a DGLM model, specifically, we have shown how to define the relationship between the latent vector $\theta_t$ and the linear predictors $\lambda_t$, along with the temporal dynamic of $\theta_t$. Now we proceed to define the observational model for $Y_t$ and the relationship between $\lambda_t$ and $\eta_t$, the parameters for the observational model.

In each subsection, we will assume that the linear predictors are already defined, along with all the structure that must come along with then, moreover, we also assume that the user have created the necessary amount of linear predictors for each type of outcome (we will see, in each case, what that amount is) and that those linear predictors were named as `lambda_1`,...,`lambda_k`.

$$
\begin{align}
Y_t|\theta_t &\sim \mathcal{F}\left(\eta_t\right),\\
g(\eta_t) &=\lambda_{t}=F_t'\theta_t.
\end{align}
$$

## Poisson case

Let's begin with the most simply (and yet non trivial) outcome offered by the package: The Poisson outcome with unknown rate.

In this case, we assume the following observational model model:

$$
\begin{align}
Y_t|\eta_t &\sim Poisson\left(\eta_t\right),\\
\ln(\eta_t) &=\lambda_{t}.
\end{align}
$$

In the notation introduced before, we have that our link function $g$ is the (natural) logarithm function.

To define such observation model, we offer the `Poisson` function, whose usage is presented bellow:

```{r eval=FALSE, include=TRUE}
Poisson(
  lambda,
  outcome,
  offset = outcome^0
)
```

As is common in the literature, we refer to the rate parameter of the Poisson distribution as `lambda` (altough, in the context of this document, this might seem confusing) and the user must provide for this argument the name of the linear predictor associated with this parameter.

For the argument `outcome` the user must provide a set of numerical values consisting of the observed values of $Y_t$ at each time. Since the $Y_t$ is a scalar for all $t$, the user can pass the outcome as a vector or as a matrix with a single column.

Lastly, the `offset` argument is optional and can be used to provide a measure of the scale of the data. If the offset is provided and is equal to $E_t$, then we will fit a model assuming that:

$$
\begin{align}
Y_t|\theta_t &\sim Poisson\left(\eta_tE_t\right),\\
\ln(\eta_t) &=\lambda_{t}.
\end{align}
$$

Now, we present a code that fits some simulated data to the described model. We use some functions described in the previous section, as well as some functions that will present later on, for now, let's focus only on the usage of the `Poisson` function.

```{r include=FALSE}
# Poisson case
T <- 200
w <- (200 / 40) * 2 * pi
data <- rpois(T, exp((sin(w * 1:T / T) + 2)))
```


```{r}
# Structure
level <- polynomial_block(rate = 1, D = 0.95)
season <- harmonic_block(rate = 1, period = 40, D = 0.98)

# Outcome
outcome <- Poisson(lambda = "rate", outcome = data)

# Fitting and ploting
fitted_data <- fit_model(level, season, outcomes = outcome)
plot(fitted_data, plot_pkg = "base")
```

In the code above, we have that the variable `data` is simply a vector of size 200 contained the simulated data.

Notice that, while creating the structure, we defined a linear predictor named `rate`, whose behavior is being explained by a first order polynomial trend and a seasonal component defined in the harmonic block. Since the value passed to `rate` equals 1 in both blocks, we have that these components have a constant effect (and equal to 1) on the linear predictor on all times, although the components themselves vary their value through time, such as to capture the behavior of the series.

Later on, when creating the outcome, we pass the name `'rate'` as the linear predictor associated with `lambda`, the rate parameter of the Poisson distribution.

This is a particularly simply usage of the package, the Poisson kernel being the one with the lesser amount of parameters. Moving forward, we will present outcomes whose specification can be a bit more complex.

## Gamma case

In this subsection we will present the Gamma case, in which we assume the following observational model:

$$
\begin{align}
Y_t|\alpha_t,\beta_t &\sim \mathcal{G}\left(\alpha_t,\beta_t\right),\\
\alpha_t&=\exp\{\lambda_{1t}\},\\
\beta_t&=\exp\{\lambda_{2t}\}
\end{align}
$$

For this outcome we have a few variations. First, there's a matter of parametrization. We allow the user to define the model by any non redundant pair of:

$$
\begin{align}
\alpha_t&,\\
\beta_t&,\\
\phi_t&=\alpha_t,\\
\mu_t&=\frac{\alpha_t}{\beta_t},\\
\sigma_t&=\frac{1}{\beta_t}.
\end{align}
$$

Naturally, the user **CANNOT** specify both $\alpha_t$ AND $\phi_t$ or $\beta_t$ AND $\sigma_t$, as such specification is redundant at best, and incoherent at worst. Outside of those cases, in which the package will raise an error, any combination can be used by the user, allowing for the structure of the model to be define with the variables that are most convenient (it may be easier or more intuitive to specify the structure in the mean $\mu_t$ and the scale $\sigma_t$, than on the shape $\alpha_t$ and rate $\beta_t$).

Another particularity of the Gamma outcome is that the user may set the shape parameter $\phi_t$ to a known constant. In that case, the user must specify the structure to the mean parameter $\mu_t$ (he is not allowed to specify neither $\beta_t$ nor $\sigma_t$). In general, it is not expected that the shape parameter is known, still, there are some important applications where it is common the use of Exponential Model ($\phi_t=1$) or some other particular cases of the Gamma distribution. The estimation of the shape parameter $\phi_t$ is still under development, as such, results may be unreliable and we will not discuss the fitting of such models in this document (a version of the package with a proper estimation for $\phi_t$ will be released very soon).

No matter the parametrization, the link function $g$ will always be the logarithm function, as such, given a certain parametrization, we can write the linear predictor of any other parametrization as a linear transformation of the original linear predictors.

In the examples of this section, we will always use the parameters $\phi_t$ (when applicable) and $\mu_t$, but the code used can be trivially adapted to other parametrizations.

```{r eval=FALSE, include=TRUE}
Gamma(
  phi = NA,
  mu = NA,
  alpha = NA,
  beta = NA,
  sigma = NA,
  outcome,
  offset = outcome^0
)
```

Similar to the Poisson case, the argument `outcome` must provide a set of numerical values consisting of the observed values of $Y_t$ at each time. Since the $Y_t$ is a scalar for all $t$, the user can pass the outcome either as a vector or as a matrix with a single column.

The `offset` argument is optional and can be used to provide a measure of the scale of the data. If the offset is provided and is equal to $E_t$, then we will fit a model assuming that:

$$
\begin{align}
Y_t|\theta_t &\sim \mathcal{G}\left(\alpha_t,\beta_t E_t^{-1}\right).
\end{align}
$$

Note that the above model implies that:

$$
\mathbb{E}[Y_t|\theta_t]=\frac{\alpha_t}{\beta_t}E_t.
$$

The arguments `phi`, `mu`, `alpha`, `beta` and `sigma` should be character strings indicating the name of the linear predictor associated with their respective linear predictor. The user may opt to pass `phi` as a positive numerical value, it that case, the shape parameter $\phi_t$ is considered known and equal to `phi` for all $t$.



```{r include=FALSE}
T <- 200
w <- (200 / 40) * 2 * pi
phi <- 2.5
mu <- exp((sin(w * 1:T / T) + 2))
data <- matrix(rgamma(T, phi, phi / mu), T, 1)

level <- polynomial_block(mu = 1, D = 0.95)
season <- harmonic_block(mu = 1, period = 40, D = 0.98)
scale <- polynomial_block(phi = 1, D = 1)
```


```{r}
outcome <- Gamma(phi = phi, mu = "mu", outcome = data)

fitted_data <- fit_model(level, season, outcomes = outcome)

plot(fitted_data, lag = -1, plot_pkg = "base")
```

## Multinomial case

In this subsection we will present the Multinomial case. Suppose that we have a sequence of $k$-dimensional non-negative integer vectors $Y_t$, such that $Y_t=(Y_{1t},...,Y_{kt})'$. We assume that:

$$
\begin{align}
Y_t|N_t,\vec{p}_t &\sim Multinom\left(N_t,\vec{p}_t\right),\\
\ln\left\{\frac{p_{it}}{p_{kt}}\right\}&=\lambda_{it}, i=1,...,k-1,\\
N_t&=\sum_{i=1}^{k}Y_{it},
\end{align}
$$
where $\vec{p}_t=(p_{1t},...,p_{kt})'$, with $p_{it} \ge 0, \forall i$ and $\sum_{i=1}^k p_{it}=1$.

Notice that $N_t$ is automatically defined by the values of $Y_t$, such that $N_t$ is always considered a known parameter. Also, it is important to point out that this model has only $k-1$ free parameters (instead of $k$), since the restriction $\sum_{i=1}^k p_{it}=1$ implies that defining $k-1$ entries of $\vec{p}_t$ defines the remaining value. Specifically, we will always take the last entry (or category) of $Y_t$ as the reference value, such that $p_{kt}$ can be considered as the baseline probability of observing data from a category (i.e., we will model how each $p_{it}$ relates to the baseline probability $p_{kt}$).

To create an outcome for this model, we can make use of the `Multinom` function:

```{r eval=FALSE, include=TRUE}
Multinom(
  p,
  outcome,
  offset = outcome^0
)
```

For the Multinomial case, `p` must be a character vector of size $k-1$ containing the names of the linear predictors associated with $\ln\left\{\frac{p_{it}}{p_{kt}}\right\}$ for $i=1,...,k-1$.

The `outcome` argument must be a $T \times k$ matrix containing the values of $Y_t$ for each observation. Notice that each line $i$ must represent the values of all categories in time $i$ and each column $j$ must represent the values of a category $j$ through time .

The `offset` argument is optional and must have the same shape of `outcome` (it's shape is also interpreted in the same manner). The argument can be used to provide a measure of the scale of the data and, if the offset is provided, such that, in each time $t$, it is equal to $E_t=(E_{1t},...,E_{kt})'$, then we will fit a model assuming that:

$$
\begin{align}
Y_t|\theta_t &\sim Multinom\left(N_t,\vec{p}^*_t\right),\\
\ln\left\{\frac{p^*_{it}}{p^*_{kt}}\right\}&=\ln\left\{\frac{p_{it}}{p_{kt}}\right\}+\ln\left\{\frac{E_{it}}{E_{kt}}\right\}, i=1,...,k-1.
\end{align}
$$

At the end of this section we present a brief discussion of implications of the inclusion of the offset and how to interpret it, as well as a explanation for the way we chose to include it.


Again, we present a brief example for the usage of this outcome:

```{r include=FALSE}
# Multinomial case
T <- 200
y1 <- rpois(T, exp(5 + (-T:T / T) * 5))
y2 <- rpois(T, exp(6 + (-T:T / T) * 5 + sin((-T:T) * (2 * pi / 12))))
y3 <- rpois(T, exp(5))

y <- cbind(y1, y2, y3)
```


```{r}
level1 <- polynomial_block(p1 = 1, order = 2)
level2 <- polynomial_block(p2 = 1, order = 2)
season <- harmonic_block(p2 = 1, period = 12)
outcome <- Multinom(p = c("p1", "p2"), outcome = y)

fitted_data <- fit_model(level1, level2, season, outcomes = outcome)
plot(fitted_data, lag = -1, plot_pkg = "base")
```

### Some comments on the usage of an offset

The model presented in this section intend to model a phenomena such that we have $N_t$ subjects that were distributed randomly (but not necessarily uniformly randomly) among $k$ categories. In this scenario, $p_{it}$ represent the probability of one observation fall within the category $i$, such that:

$$
p_{it}=\mathbb{P}(Y_{it}=1|N_t=1).
$$

In some application, it might be the case that $N_t$ represents the counting of some event of interest and we want to model the probability of this event occurring in each category. In this scenario, it is not clear how to use the multinomial model, since we will have that:

$$
p_{it}=\mathbb{P}(\text{Observation belong to category }i|\text{Event occured}),
$$
but we actually want to known:

$$
p^*_{it}=\mathbb{P}(\text{Event occured}|\text{Observation belong to category }i).
$$

Notice that we can write:

$$
\begin{aligned}
p^*_{it}&=\mathbb{P}(\text{Event occured}|\text{Observation belong to category }i)\\
&=\frac{\mathbb{P}(\text{Observation belong to category }i|\text{Event occured})\mathbb{P}(\text{Event occured})}{\mathbb{P}(\text{Observation belong to category }i)}\\
&=\frac{p_{it}\mathbb{P}(\text{Event occured})}{\mathbb{P}(\text{Observation belong to category }i)}.
\end{aligned}
$$

The above relation implies that:

$$
\begin{aligned}
\ln\left\{\frac{p^*_{it}}{p^*_{kt}}\right\}
&=\ln\left\{\frac{p_{it}}{p_{kt}}\right\}-\ln\left\{\frac{\mathbb{P}(\text{Observation belong to category }i)}{\mathbb{P}(\text{Observation belong to category }k)}\right\}.
\end{aligned}
$$

If we pass to the `offset` argument of the `Multinom` function such that $E_{t} \propto (\mathbb{P}(\text{Observation belong to category }1),...,\mathbb{P}(\text{Observation belong to category }k))'$, then, by the specification provided in this section, we have that:

$$
\ln\left\{\frac{p^*_{it}}{p^*_{kt}}\right\}=\lambda_{it},
$$
in other words, the linear predictors (and consequently, the model structure) will model the probability that a event occur in a specific class (instead of the probability that an observation belongs to a class, given that the event occurred).

To obtain $\frac{p^*_{it}$ itself (i.e. the probability of the event occuring given the observation belongs to the category $i$), one can use Bayes formula, as long as $\mathbb{P}(\text{Event occured})$ is known. Indeed, one can write:

$$
\begin{aligned}
p^*_{it}&=p_{it}\frac{\mathbb{P}(\text{Event occured})}{\mathbb{P}(\text{Observation belong to category }i)}\\
        &=\frac{exp\{\lambda_i\}}{1+\sum_j exp\{\lambda_j\}}\frac{\mathbb{P}(\text{Event occured})}{\mathbb{P}(\text{Observation belong to category }i)}
\end{aligned}
$$

## Normal case

In some sense, we can think of this as the most basic case, at least in a theoretical point of view, since the Kalman Filter was develop for this specific scenario. Indeed, if we have a **static** observational variance/covariance matrix (even if unkown), we fall within the DLM class, which has exact analitical solution. Yet, our package we goes a step further, offering the possibility of temporal dynamic for both the mean and the observational variance/covariance matrix, allowing even the inclusion of dynamic regressions, seasonal patterns, auto regressive components, etc.

We will present this case in two contexts: the first, which is a simply implementation of the Kalman Filter and Smoother, deals with data coming from an Normal distribution (possibly multivariated) with unknown mean and known variance/covariance matrix: the second deals with data coming from a univariate Normal distribution with unknown mean and unknown variance.

Also, at the end of the second subsection, we present an extension to the bivariated Normal distribution with unknown mean and unknown covariance matrix. A study is being conducted to expand this approach to the $k$-variated case, for any arbitrary $k$.

### Normal outcome with known variance

Similarly to the Multinomial case, suppose that we have a sequence of $k$-dimensional vectors $Y_t$, such that $Y_t=(Y_{1t},...,Y_{kt})'$. We assume that:

$$
\begin{align}
Y_t|\mu_t,V &\sim \mathcal{N}_k\left(\mu_t,V\right),\\
\mu_{it}&=\lambda_{it}, i=1,...,k,\\
\end{align}
$$
where $V$ is a known symmetric, definite positive $k\times k$ matrix. Also, for this model, we assume that the link function $g$ is the identity function.

To create an outcome for this model, we can make use of the `Normal` function:

```{r eval=FALSE, include=TRUE}
Normal(
  mu,
  V = NA,
  Tau = NA,
  Sigma = NA,
  Sd = NA,
  outcome,
  offset = outcome^0
)
```

Intuitively, the `mu` argument must be a character vector of size $k$ containing the names of the linear predictors associated with each $\mu_{i.}$. The user must also specify one (and only one) of `V`, `Tau`, `Sigma` or `Sd`. If the user provides `V` or `Sigma`, $V$ is assumed to be the passed value; if the user provides `Tau`, $V$ is assumed to be the inverse of the passed matrix (i.e., `Tau` is the precision matrix); if the user provides `Sd`, $V$ is assumed to be such that the standard deviation of the observations is equal to diagonal of `Sd` and the correlation between observations is assumed the be equal to the off-diagonal elements of `Sd`. 

The `outcome` argument must be a $T \times k$ matrix containing the values of $Y_t$ for each observation. Notice that each line $i$ must be the values of all categories in time $i$ and each column $j$ must represent the values of a category $j$ through time .

The `offset` argument is optional and must have the same shape of `outcome` (it's shape is also interpreted in the same manner). The argument can be used to provide a measure of the scale of the data and, if the offset is provided, such that for each time $t$ it is equal to $E_t=(E_{1t},...,E_{kt})'$, then we will fit a model assuming that:

$$
\begin{align}
Y_t|\mu_t,V^*_t &\sim \mathcal{N}_k\left(\mu_t,V^*_t\right),\\
\mu_{it}&=\lambda_{it}E_{it}, i=1,...,k-1,\\
V^*_t&=\begin{bmatrix}E_{1t} & & 0\\& \ddots & \\0 & & E_{kt}\end{bmatrix}V\begin{bmatrix}E_{1t} & & 0\\& \ddots & \\0 & & E_{kt}\end{bmatrix}\\
\end{align}
$$

Next, we present two brief examples for the usage of this outcome: One for a univariate outcome and the other for a multivariate model.

```{r include=FALSE}
T <- 200
mu <- rnorm(T, 0, 0.1)
data <- rnorm(T, cumsum(mu))
```


```{r}
level <- polynomial_block(mu = 1, D = 0.95)
outcome <- Normal(mu = "mu", V = 1, outcome = data)

fitted_data <- fit_model(level, outcomes = outcome)

plot(fitted_data, lag = -1, plot_pkg = "base")
```

```{r include=FALSE}
T <- 200
mu1 <- cumsum(rnorm(T, 0, 0.1)) + 1
mu2 <- cumsum(rnorm(T, 0, 0.2)) - 1
V <- matrix(c(1, 1, 1, 4), 2, 2)
C <- var_decomp(V)
data <- matrix(rnorm(2 * T), T, 2) %*% C + cbind(mu1, mu2)
```


```{r}
level <- polynomial_block(mu = 1, D = 0.95) * 2
outcome <- Normal(
  mu = c("mu_1", "mu_2"),
  V = matrix(c(1, 1, 1, 4), 2, 2),
  outcome = data
)

fitted_data <- fit_model(level, outcomes = outcome)

plot(fitted_data, lag = -1, plot_pkg = "base")
```

### Univariated Normal outcome with unknown variance

For this type of outcome, we assume that:

$$
\begin{align}
Y_t|\mu_t,\tau_t &\sim \mathcal{N}\left(\mu_t,\tau_t^{-1}\right),\\
\mu_{t}&=\lambda_{1t},\\
\ln\{\tau_{t}\}&=\lambda_{2t}.\\
\end{align}
$$

To create an outcome for this model, we also make use of the `Normal` function:

```{r eval=FALSE, include=TRUE}
Normal(
  mu,
  Tau = NA,
  Sigma = NA,
  Sd = NA,
  outcome,
  offset = outcome^0
)
```

Just as before, the `mu` argument must be a character representing the name of the linear predictor associated with $\mu_t$. The user must also specify one (and only one) of `V`, `Tau`, `Sigma` or `Sd`, which must a character representing the name of the linear predictor associated with $\tau_t^{-1}$, $\tau_t$, $\tau_t^{-1}$ or $\tau_t^{-1/2}$, respectively.

Similar to the Gamma case, we allow multiple parametrizations for the variance of the observational model. Specifically, if the user provides `V` or `Sigma`, we assume that $\lambda_{2t}=\ln\{\sigma^2_{t}\}=-\ln\{\tau_t\}$; if the user provides `Sd`, we assume that $\lambda_{2t}=\ln\{\sigma_{t}\}=-\ln\{\tau_t\}/2$; if the user provides `Tau`, then the default parametrization is used, i.e., $\lambda_{2t}=\ln\{\tau_t\}$. 

The `outcome` argument must be a $T \times 1$ matrix containing the values of $Y_t$ for each observation. If `outcome` is a vector, we assume that $Y_t$ is univariated and each coordenate of `outcome` represents the observed value at each time. 

The `offset` argument is optional and must have the same shape of `outcome` (it's shape is also interpreted in the same manner). The argument can be used to provide a measure of the scale of the data and, if the offset is provided, such that for each time $t$ it is equal to $E_t$, then we will fit a model assuming that:

$$
\begin{align}
Y_t|\mu_t,\tau^{-1}_t &\sim \mathcal{N}_k\left(\mu_t,\tau^{-1}_t\right),\\
\mu_{t}&=\lambda_{1t}E_{t},\\
\ln(\tau_t)&=\lambda_{2t}-2\ln(E_{t})\\
\end{align}
$$

Next, we present a brief examples for the usage of this outcome:

```{r include=FALSE}
T <- 200
mu <- rnorm(T, 0, 0.1)
data <- rnorm(T, cumsum(mu))
```


```{r}
level <- polynomial_block(mu = 1, D = 0.95)
scale <- polynomial_block(tau = 1, D = 1)
outcome <- Normal(mu = "mu", Tau = "tau", outcome = data)

fitted_data <- fit_model(
  level, scale,
  outcomes = outcome
)

plot(fitted_data, lag = -1, plot_pkg = "base")
```


Currently, we also support models with bivariate Normal outcomes. In this scenario we assume the following model:

$$
\begin{align}
Y_t|\mu_{t},VV_t &\sim \mathcal{N}_2\left(\mu_t,V_t\right),\\
\mu_t&=\begin{bmatrix}\mu_{1t}\\ \mu_{1t}\end{bmatrix},\\
V_t&=\begin{bmatrix}\tau_{1t}^{-1} & \sqrt{\tau_{1t}\tau_{2t}}\rho\\ \sqrt{\tau_{1t}\tau_{2t}}\rho & \tau_2^{-1}\end{bmatrix},\\
\mu_{it}&=\lambda_{it}, i=1,2\\
\tau_{it}&=\ln\{\lambda_{(i+2)t}\}, i=1,2\\
\rho_{t}&=\tanh\{\lambda_{5t}\}.\\
\end{align}
$$
Notice that $\rho_t$ represents the correlation between the series at time $t$ and, to guarantee that $\rho_t \in (-1,1)$, we use the Inverse Fisher transformation (also known as the hyperbolic tangent) as link function.

For those models, `mu` must be a character vector, similarly to the case where $V$ is known, and `V`, `Tau`, `Sigma` or `Sd` must be a $2 \times 2$ character matrix. The diagonal elements are interpreted as the linear predictors associated with the precisions, variances or standard deviations, repectively, while the off diagonal elements must be equals (one of then can be equal to `NA`) and will be interpreted as the linear predictor associated with $\rho_t$.

Bellow we present an usage example for the bivariate case:

```{r include=FALSE}
T <- 200
mu1 <- cumsum(rnorm(T, 0, 0.1)) + 1
mu2 <- cumsum(rnorm(T, 0, 0.2)) - 1
V <- matrix(c(1, 1, 1, 4), 2, 2)
data <- matrix(rnorm(T * 2), T, 2) %*% chol(V) + cbind(mu1, mu2)
var(data)
```


```{r eval=FALSE, include=FALSE}
level <- polynomial_block(mu = 1, D = 0.95, C0 = 1) * 2 +
  polynomial_block(sigma = 1, D = 1, C0 = 1) * 2 +
  polynomial_block(rho = 1, D = 1, C0 = 1)

outcome <- Normal(
  mu = c("mu_1", "mu_2"),
  V = matrix(c("sigma_1", "rho", "rho", "sigma_2"), 2, 2),
  outcome = data
)

fitted_data <- fit_model(level, outcomes = outcome)

plot(fitted_data, lag = -1, plot_pkg = "base")
```

# Advanced examples

## Tools for sensibility analysis

# Modeling multiple time series

# Intervention and monitoring

# References

```{r eval=FALSE, include=FALSE}
rmarkdown::render("vignettes/vignette.Rmd")
```

