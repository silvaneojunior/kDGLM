---
title: "kDGLM: an R package for Bayesian analysis of Dynamic Generialized Linear Models"
author: Silvaneo V. dos Santos Jr.
output: rmdformats::readthedown
date: "`r Sys.setlocale('LC_TIME', 'English'); format(Sys.Date(),'%d of %B, %Y')`"
bibliography: '`r system.file("REFERENCES.bib", package="kDGLM")`'
link-citations: TRUE
urlcolor: blue
linkcolor: green
vignette: >
  %\VignetteIndexEntry{kDGLM: an R package for Bayesian analysis of Dynamic Generialized Linear Models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = ""
)

# library(kDGLM)
devtools::load_all()
set.seed(13031998)
```

# Introduction

This vignette is intended as an introduction to the usage of the kDGLM package, which offers routines for Bayesian analysis of Dynamic Generalized Linear Models, including fitting (filtering and smoothing), forecasting, sampling, intervention and automated monitoring, following the theory developed and/or explored in @Kalman_filter_origins, @WestHarr-DLM and @ArtigokParametrico.

In this document we will focus exclusively in the usage of the package and will only briefly mention the theory behind these models and only with the intention of highlighting the notation. We highly recommend all users to read the theoretical work in which we based this package.

This document is organized in the following order: 

1. First we introduce the notations and the class of models we will be dealing with;
3. Next we present the details about the specification of the model structure, offering tools that allow for an easy, fast and (hopefully) intuitive way of defining models;
4. In the following section we discuss details about how the user can specify the observational model;
2. Then we present some basic examples of model fitting, also showing the auxiliary functions that help the user to analyse the fitted model. We also show tools for easy model selection;
5. Lastly, we present a variety of advanced examples, combining the basic features shown in previous sections to create more complex models;

# Notation

Let us assume that the user is interested in analyzing a Time Series $\{Y_t\}_{t=1}^T$ which can be described by the following model:

$$
\begin{align}
Y_t|\eta_t &\sim \mathcal{F}\left(\eta_t\right),\\
g(\eta_t) &=\lambda_{t}=F_t'\theta_t,\\
\theta_t&=G_t\theta_{t-1}+\omega_t,\\
\omega_t &\sim \mathcal{N}_n(h_t,W_t),
\end{align}
$$
where:

- $\theta_t$ are the unknown parameter of interest;
- $\mathcal{F}$ is a probability distribution in the Exponential Family and indexed by $\eta_t$;
- $g$, called the link function, is a pre-specified function (in this package, each choice of $\mathcal{F}$ will have a pre-determined $g$);
- $F_t$, called the design matrix, is a (mostly) known matrix specified by the user;
- $G_t$, called the evolution matrix, is a (mostly) known matrix specified by the user;
- $h_t$, called drift, is a known vector specified by the user. In general, we have $h_t=0$, except when making interventions on the model [see @WestHarr-DLM, chapter 11];
- $W_t$ is a known covariance matrix specified by the user;
- $n$ is the dimension of $\theta_t$ (we will always consider that $\theta_t$ has the same dimension for all $t$).

Also, following the notation in @WestHarr-DLM, we will define $\mathcal{D}_t$ as the information one has after the first $t$ observations, such that $\mathcal{D}_t=\mathcal{D}_{t-1}\cup\{Y_t\}$ (for now, let us assume that there is no external source of information beside $Y_t$ itself) and $\mathcal{D}_0$ denotes the information we have about the process $\{Y_t\}^T_{t=1}$ *prior* to observing the data.

For the specification of $W_t$, we follow the ideas presented @WestHarr-DLM, section 6.3, such that we define $W_t=Var[G_t\theta_{t-1}|\mathcal{D}_{t-1}] \odot (1-D_t) \oslash D_t + H_t$, where:

- $D_t$ (called the discount matrix) is a matrix of values between $0$ and $1$ (including $1$, but excluding $0$);
- $\odot$ is the Hadamard product (i.e., element-wise product);
- $\oslash$ is the Hadamard division (i.e., element-wise division);
- $H_t$ is a known matrix specified by the user.
 
Notice that this specification implies that $Var[\theta_{t}|\mathcal{D}_{t-1}]=Var[G_t\theta_{t-1}|\mathcal{D}_{t-1}] \oslash D_t + H_t$, such that, if all entries of $D_t$ are equal to $1$, then $Var[\theta_{t}|\mathcal{D}_{t-1}]=Var[G_t\theta_{t-1}|\mathcal{D}_{t-1}]+H_t$. If also all entries of $H_t$ are equal to $0$, then $Var[\theta_{t}|\mathcal{D}_{t-1}]=Var[G_t\theta_{t-1}|\mathcal{D}_{t-1}]$ (i.e. $\theta_t$ is a deterministic transformation of $\theta_{t-1}$), such that our model becomes equivalent to a Generalized Linear Model.

Notice that we **did not** make any assumptions on the dimension of $Y_t$ or $\eta_t$, indeed, the range of applications of this package covers both time series of vectors (as in the Multinomial case) and/or with multiple unknown parameters (as in the Normal with unknown variance case).

Naturally, the manual specification of all the structural components of the model can be quite tiresome. With that in mind, our package offers a wide and ever expanding set of auxiliary functions that aim to help the user to specify the structure of a model. In the next section we will explore some of those tools.

# Creating the model structure

In this sections we will discuss the specification of the model structure. We will consider the structure of a model as all the elements that determine the relation between our linear predictor $\lambda_t$ and our latent states $\theta_t$ though time, i.e., we want the define the following, highlighted equations from the model:

$$
\require{color}
\begin{align}
Y_t|\eta_t &\sim \mathcal{F}\left(\eta_t\right),\\
g(\eta_t) &= {\color{red}\lambda_{t}=F_t'\theta_t,}\\
{\color{red}\theta_t }&{\color{red}=G_t\theta_{t-1}+\omega_t,}\\
{\color{red}\omega_t }&{\color{red}\sim \mathcal{N}_n(h_t,W_t)}.
\end{align}
$$

Namely, we consider that the structure of a model consists of the matrices $F_t$, $G_t$, $h_t$, $H_t$ and $D_t$.

Although we allow the user to manually define each entry of each of those matrices (which we **do not** recommend), we also offer tools to simplify this task. Let us start by presenting the basis function for all structural blocks, the `polynomial_block` function.

## A structure for polynomial trend models

```{r eval=FALSE, include=TRUE}
polynomial_block(
  ...,
  order = 1,
  name = "Var.Poly",
  D = 1,
  h = 0,
  H = 0,
  a1 = 0,
  R1 = c(9, rep(1, order - 1)),
  monitoring = c(TRUE, rep(FALSE, order - 1))
)
```

This function will create a structural block based on @WestHarr-DLM, chapter 7, i.e., it creates a latent vector $\theta_t=(\theta_{1,t},...,\theta_{k,t})'$, where $k$ is the order of the polynomial block, such that:

$$
\begin{align}
\theta_{i,t} &= \theta_{i,t-1}+\theta_{i+1, t-1}+\omega_{i,t}, i=1,...,k-1\\
\theta_{k,t} &= \theta_{k,t-1}+\omega_{k,t},\\
\theta_1&\sim \mathcal{N}_k(a_1,R_1),\\
\omega_{1,t},...,\omega_{k,t}&\sim \mathcal{N}_k(h_t,W_t),
\end{align}
$$
where $W_t=Var[\theta_t|\mathcal{D}_{t-1}]\odot (1-D_t) \oslash D_t+H_t$.

Notice that the user does not need to specify the matrix $G_t$, since it is implicitly determined by the equations above and the order of the polynomial block. Each type of block will define it own matrix $G_t$, as such, the user does not need to worry about $G_t$, except in very specific circumstances, where one need a type of model that is not yet implemented in the package.

It is easy to see the correspondence between most of the arguments of the `polynomial_block` function and their respective meaning in the block specification, remaining only to explain the use of the `...`, `name` and `monitoring` arguments. We do advise all users to consult the associated documentation for more details (see `help(polynomial_block)` or the reference manual).

The argument `...` is used to specify the matrix $F_t$, specifically, the user must provide a list of named values, whose name indicate a linear predictor $\lambda_t$ and its associated value represents the effect of $\theta_{1,t}$ in this predictor (the other latent states are assumed to have no effect on the linear predictors). Naturally, if a linear predictor is not present in `...`, it is understood that $\theta_{1,t}$ have no effect whatsoever on that particular linear predictor. Such specification of $F_t$ may seem strange, but as the reader will see further bellow, this way of specifying $F_t$ is very useful to avoid confusion when dealing with multiple linear predictors.

It is important to emphasize that the dimesion of $\theta_t$ is implicitly determined by the block structure, besides, $\lambda_t$ is implicitly determined by `...`, since one linear predictor will be created for each **unique** named value passed by the user.

The argument `name` is optional and it is used to help the user to identify each latent state after fitting the model, specifically, each latent state will be called by its own name when plotting or printing $\theta_t$.

Lastly, the `monitoring` shall be explained latter, in the section about monitoring and intervation.

Notice that the default values of each argument leads to a first order polynomial block with no temporal dynamic, besides, the default prior is such that $\theta_{1,1} \sim \mathcal{N}(0,9)$ and $\theta_{i,1} \sim \mathcal{N}(0,1), i=2,...,k$.

To exemplify the usage of this function, let us assume that we have a simple Normal model with known variance $\sigma^2$, in which $\eta$ is the mean parameter and the link function $g$ is such that $g(\eta)=\eta$. Let us also assume that the mean is constant over time and we have no explanatory variables, such that our model can be written simple as:

$$
\begin{align}
Y_t|\theta_t &\sim \mathcal{N}_1\left(\eta_t, \sigma^2\right),\\
\eta_t &=\lambda_{t}=\theta_t,\\
\theta_t&=\theta_{t-1}=\theta.
\end{align}
$$

In this case, we have that $F_t=1$, $G_t=1$, $D_t=1$, $h_t=0$ and $H_t=0$, for all $t$. Assuming a prior distribution $\mathcal{N}(0,9)$ for $\theta$, we can create such structure using the following code:

```{r eval=FALSE, include=TRUE}
mean_block <- polynomial_block(
  eta = 1,
  order = 1,
  name = "Mean",
)
```

By setting `eta=1`, we specify that there is a linear predictor called *eta*, and that $eta = 1 \times \theta$. By setting `order = 1`, we specify that $\theta_t$ is a scalar and that $G_t=1$. We can omit the values of `a1` , `R1`, `D`, `h` and `H`, since the default values are equal to the desired. We could also omit the argument `order`, since the default is already $1$, but we chose to explicit define it so as to emphasize its usage.

Suppose now that we have a explanatory variable $X$ that we would like to introduce in our model to help explain the behavior of $\eta_t$. We could similarly define such structure by creating an additional block such as:

```{r eval=FALSE, include=TRUE}
polynomial_block(
  eta = X,
  name = "Var X"
)
```

By setting `eta=X`, we specify that there is a linear predictor called *eta*, and that $eta = X \times \theta$. If $X=(X_1,...,X_T)'$ is a vector, then we would have $F_t=X_t$, for each $t$, such that $\eta_t = X_t \times \theta_t$.

It is worth noting that there is a specific structural block designed for regressions, called `regression_block`, but we also allow any structural block to be used as a regression, allowing the user to specify complex temporal dynamics for the effects of any covariate.

Until now, we only discussed the creation of static models, but the inclusion of temporal dynamic is very straightforward, one must simply specify the values of `H` to be greater than $0$ and/or the values of `D` to be lesser than $1$:

```{r eval=FALSE, include=TRUE}
mean_block <- polynomial_block(
  eta = 1,
  order = 1,
  name = "Mean",
  D = 0.95
)
```

Bellow we present a plot of two models fitted to the same data: one with a static mean and another using a dynamic mean.

```{r echo=FALSE, results='hide'}

# Normal case
T <- 200
mu <- rnorm(T, 0, 0.5)
data <- rnorm(T, cumsum(mu))

level1 <- polynomial_block(
  mu1 = 1,
  D = 1,
  name = "Static mean"
)
level2 <- polynomial_block(
  mu2 = 1,
  D = 0.95,
  name = "Dynamic mean"
)
# Known variance
Static.mean <- Normal(mu = "mu1", Sigma = 1, data = data)
Dynamic.mean <- Normal(mu = "mu2", Sigma = 1, data = data)

fitted.data <- fit_model(level1, level2,
  Static.mean = Static.mean,
  Dynamic.mean = Dynamic.mean
)

plot(fitted.data, lag = -1, plot.pkg = "base")
```

The detailed theory behind the structure discussed in this section can be found in chapters 6, 7 and 9 from @WestHarr-DLM.

## A structure for dynamic regression models

```{r eval=FALSE, include=TRUE}
regression_block(
  ...,
  max.lag = 0,
  zero.fill = TRUE,
  name = "Var.Reg",
  D = 1,
  h = 0,
  H = 0,
  a1 = 0,
  R1 = 9,
  monitoring = rep(FALSE, max.lag + 1)
)
```

The `regression_block` function creates a structural block for a dynamic regression with covariate $X_t$, as specified in @WestHarr-DLM, chapter 9. When `max.lag` is equal to $0$, this function can be see as a wrapper for the `polynomial_block` function with order equal to $1$. When `max.lag` is greater or equal to $1$, the `regression_block` function is equivalent to the superposition of several `polynomial_block` functions with order equal to $1$. Specifically, if the linear predictor $\lambda_t$ is associated with this block, we can describe its structure with the following equations:

$$
\begin{align}
\lambda_t&=\sum_{i=0}^{max.lag}X_{t-i}\theta_{i,t},\\
\theta_{i,t}&=\theta_{i,t-1}+\omega_{i,t},\quad \forall i,\\
\omega_{0,t},...,\omega_{max.lag,t}&\sim \mathcal{N}_{max.lag+1}(0,W_t),\\
\theta_{0,1},...,
\theta_{max.lag,1}&\sim \mathcal{N}_{max.lag+1}(a_1,R_1),
\end{align}
$$
where $W_t=Var[\theta_t|\mathcal{D}_{t-1}]\odot (1-D_t) \oslash D_t+H_t$.

The usage of the `regression_block` function is quite similar to that of the `polynomial_block` function, the only differences being in the `max.lag` and `zero.fill` arguments. The `max.lag` defines the maximum lag of the variable $X_t$ that has effect on the linear predictor. For example, if we define `max.lag` as $3$, we would be defining that $X_t$, $X_{t-1}$, $X_{t-2}$ and $X_{t-3}$ all have an effect on $\lambda_t$, such that $max.lag+1$ latent variables are created, each one representing the effect of a lagged value of $X_t$.

Lastly, the `zero.fill` argument defines if the package should take the value of $X_t$ to be $0$ when $t$ is non-positive, i.e., if `TRUE` (default), the package considers $X_t=0$, for $t=0,-1,...,-max.lag+1$. If `zero.fill` is `FALSE`, then the user must provide the values of $X_t$ as a vector of size $T+max.lag$ (instead of $T$), where $T$ is the length of the time series that is being modeled, and the first $max.lag$ values of that vector will be taken as $X_{-max.lag+1},...,X_0$.

The usage of the remaining arguments is identical to that of the `polynomial_block` function, and can also be inferred by the previous equation. As such, we will only emphasize the usage of the `...` argument. 

The argument `...` is used to specify the matrix $F_t$, specifically, the user must provide a list of named values, whose name indicate a linear predictor $\lambda_t$. The effect of $X_t$ in this predictor is represented by $\theta_{0,t},...,\theta_{max.lag,t}$, such that $\theta_{i,t}$ represents the effect of $X_{t-i}$ in $\lambda_t$ (i.e., the lagged effect of order $i$). It is easy to see that, if $X_t=x$, for all $t$ (in other words, if $X_t$ is constant), one **should not** use $max.lag$ greater then $0$ . Naturally, if a linear predictor is not present in `...`, it is understood that $X_t$ has no effect whatsoever on that particular linear predictor. Such specification of $F_t$ may seem strange, but as the reader will see further bellow, this way of specifying $F_t$ is very useful to avoid confusion when dealing with multiple linear predictors.

Here we present the code for fitting the following model:

$$
\begin{align}
Y_t|\theta_t &\sim Poisson\left(\eta_t\right),\\
\ln(\eta_t) &=\lambda_{t}=X_t\theta_t,\\
\theta_t&=\theta_{t-1}+\omega_t,\\
\omega_t &\sim \mathcal{N}_1(0,W_t),
\end{align}
$$
where $X_t$ is a known covariate and $W_t$ is specified using a discount factor of $0.95$.

```{r include=FALSE}
T <- 200
X <- rgamma(T, 20, 20 / 5)
sd_gamma <- 0.5 / (2 * sqrt(T))
gamma_coef <- 0.5 + cumsum(rnorm(T, 0, 0.1 / (2 * sqrt(T))))
data <- rpois(T, exp(gamma_coef * X))
```


```{r echo=TRUE}
regression <- regression_block(The_name_of_the_linear_predictor = X, D = 0.95)

outcome <- Poisson(lambda = "The_name_of_the_linear_predictor", data = data)

fitted.data <- fit_model(regression, outcome)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(gamma_coef,type='l',lty=2,col='red',ylim=c(0.45,0.55),ylab=expression(theta[t]),xlab = 'Time',main=expression(paste('Estimation of ',theta[t])))
lines(fitted.data$mts[1,])
lines(fitted.data$mts[1,]-1.96*sqrt(fitted.data$Cts[1,1,]),lty=2)
lines(fitted.data$mts[1,]+1.96*sqrt(fitted.data$Cts[1,1,]),lty=2)
legend('topright',legend=c('True value','Mean','C.I. 95%'),lty=c(2,1,2),col=c('red','black','black'))
```

The detailed theory behind the structure discussed in this section can be found in chapters 6 and 9 from @WestHarr-DLM.

## A structure for harmonic trend models

```{r eval=FALSE, include=TRUE}
harmonic_block(
  ...,
  period,
  order = 1,
  name = "Var.Sazo",
  D = 1,
  h = 0,
  H = 0,
  a1 = 0,
  R1 = 4,
  monitoring = rep(FALSE, order * 2)
)
```

This function will creates a structural block based on  @WestHarr-DLM, chapter 8, i.e., it creates a latent vector $\theta_t=(\theta_{1,t},\theta_{2,t},...,\theta_{2\times order-1,t},\theta_{2\times order,t})'$, so that:

$$
\begin{bmatrix}\theta_{2i -1,t}\\ \theta_{2i,t}\end{bmatrix} = \begin{bmatrix}cos(iw) & sin(iw)\\ -sin(iw) & cos(iw)\end{bmatrix}\begin{bmatrix}\theta_{2i -1,t-1}\\ \theta_{2i,t-1}\end{bmatrix}+\begin{bmatrix}\omega_{2i -1,t}\\ \omega_{2i,t}\end{bmatrix}, i=1,...,order\\
\begin{align}
\theta_{1,1},...,\theta_{2 \times order,1}&\sim \mathcal{N}_{2\times order}(a_1,R_1),\\
\omega_{1,t},...,\omega_{2 \times order,t}&\sim \mathcal{N}_{2\times order}(0,W_t),\\
\end{align}
$$
where $W_t=Var[\theta_t|\mathcal{D}_{t-1}]\odot (1-D_t) \oslash D_t+H_t$ and $w=\frac{2\pi}{period}$.

Notice that the user does not need to specify the matrix $G_t$, since it is implicitly determined by the order and the period of the harmonic block, being a block diagonal matrix where each block is a rotation matrix for an angle multiple of $w$, such that, if `period` is an integer, $G_t^{period}=I$. Notice that, when `period` is an integer, it represents the length of the seasonal cycle. For instance, if we have a time series with monthly observations and we believe this series to have an annual pattern, then we would set the `period` for the harmonic block to be equal to 12 (the number of observations until the cycle "resets"). For details about the order of the harmonic block and the representation of seasonal patterns with Fourier Series, see  @WestHarr-DLM, chapter 8.

Similar to the `polynomial_block` function, the argument `...` is used to specify the matrix $F_t$. The user must provide a list of named values whose name indicate a linear predictor $\lambda_t$ and its associated value represent the effect of $\theta_{it}$, for odd $i$, in this predictor (the other latent states are assumed to have no effect on the linear predictors). Naturally, if a linear predictor is not present in `...`, it is understood that this block has no effect whatsoever on that particular linear predictor.

The natural usage of this block is for specifying harmonic trends for the model, but it can also be used for explanatory variables with seasonal effect on the linear predictor, for that, see the usage of the `regression_block` and `polynomial_block` functions.

Here we present a simply usage example for a harmonic block with period $12$:

```{r eval=FALSE, include=TRUE}
mean_block <- harmonic_block(
  eta = 1,
  period = 12,
  D = 0.975
)
```

Bellow we present a plot of a Poisson model with such structure:

```{r include=FALSE}
# Poisson case
T <- 60
w <- 2 * pi / 12
data <- rpois(T, exp(1.5 * cos(w * 1:T)))
```


```{r echo=FALSE, results='hide'}

season <- harmonic_block(rate = 1, period = 12, D = 0.975)

outcome <- Poisson(lambda = "rate", data = data)

fitted.data <- fit_model(season, outcome)

plot(fitted.data, lag = -1, plot.pkg = "base")
```


The detailed theory behind the structure discussed in this section can be found in chapters 6, 8 and 9 from @WestHarr-DLM.

## A structure for autoregresive models

```{r eval=FALSE, include=TRUE}
AR_block(
  ...,
  order,
  noise.var = NULL,
  noise.disc = NULL,
  pulse = 0,
  name = "Var.AR",
  AR.support = "free",
  a1 = 0,
  R1 = 9,
  h = 0,
  monitoring = TRUE,
  D.coef = 1,
  h.coef = 0,
  H.coef = 0,
  a1.coef = c(1, rep(0, order - 1)),
  R1.coef = c(1, rep(0.25, order - 1)),
  monitoring.coef = rep(FALSE, order),
  a1.pulse = 0,
  R1.pulse = 9,
  D.pulse = 1,
  h.pulse = 0,
  H.pulse = 0,
  monitoring.pulse = NA
)
```

This function creates a structural block based on  @WestHarr-DLM, chapter 9, i.e., it creates a latent state vector $\theta_t$, an autoregressive (AR) coefficient vector $\phi_t=(\phi_{1,t},...,\phi_{order, t})'$ and a pulse coefficient vector $\rho_t=(\rho_{1,t},...,\rho_{l,t})'$, where $l$ is the number of pulses (discussed later on) so that:

$$
\begin{align}
\theta_{t} &= \sum_{i=1}^{k}\phi_{i,t}\theta_{t-i}+\sum_{i=1}^{l}\rho_{i,t}X_{i,t}+\omega_{t},\\
\phi_{i,t}&=\phi_{i,t-1}+\omega^{\text{coef}}_{i,t},\\
\rho_{i,t}&=\rho_{i,t-1}+\omega_{i,t}^{pulse},\\
\omega_{t}&\sim \mathcal{N}_1(h_t,W_t),\\
\omega_{t}^{\text{coef}}&\sim \mathcal{N}_k(h_t^{\text{coef}},W_t^{\text{coef}}),\\
\omega_{t}^{pulse}&\sim \mathcal{N}_l(h_t^{pulse},W_t^{pulse}),\\
\theta_1&\sim \mathcal{N}(a_1,R_1),\\
\phi_1&\sim \mathcal{N}_k(a_1^{\text{coef}},R_1^{\text{coef}}),\\
\rho_1&\sim \mathcal{N}_l(a_1^{pulse},R_1^{pulse}).
\end{align}
$$
where:

$$
\begin{align}
W_t&=noise.var&+&\frac{(1-noise.disc)}{noise.disc}Var[\theta_t|\mathcal{D}_{t-1}] & &  & & ,\\
W_t^{\text{coef}}&=H_t^{\text{coef}}&+&Var[\phi_t|\mathcal{D}_{t-1}] &\odot& (1-D_t^{\text{coef}}) &\oslash& D_t^{\text{coef}},\\ W^{pulse}_t&=H_t^{pulse}&+&Var[\rho_t|\mathcal{D}_{t-1}] &\odot&(1-D_t^{pulse}) &\oslash&D_t^{pulse},
\end{align}
$$
and $X$, called pulse matrix, is a known $T \times l$ matrix.

Notice that the user does not need to specify the matrix $G_t$, since it is implicitly determined by the order of the AR block and the equations above, although, as the reader might have noticed, that evolution will always be non-linear. Since the method used to fit models in this package requires a linear evolution, we use the approach described in @WestHarr-DLM, chapter 13, to linearize the previous evolution equation. For more details about the usage of autoregressive models in the context of DLM's, see @WestHarr-DLM, chapter 9.

It is easy to understand the meaning of most arguments of the `AR_block` function based on the previous equations, but some explanation is still needed for the `...`, `name` and `AR.support` arguments, plus the arguments related with the so called *pulse*. We do advise all users to consult the associated documentation for more details (see `help(AR_block)` or the reference manual).

Similar to the `polynomial_block` function, the argument `...` is used to specify the matrix $F_t$. The user must provide a list of named values, whose name indicate a linear predictor $\lambda_t$ and its associated value represents the effect of $\theta_{t}$ in this predictor (we assume that $\phi_t$ and $\rho_t$ have no effect on the linear predictors). Naturally, if a linear predictor is not present in `...`, it is understood that $\theta_{t}$ has no effect whatsoever on that particular linear predictor.

Also, in the same fashion as the other structure functions, the `name` argument is optional, providing an easy way to identify each latent variable when plotting and summarizing a fitted model.

The `AR.support` is a character string, either `"constrained"` or `"free"`. If `AR.support` is `"constrained"`, then the AR coefficients $\phi_t$ will be forced to be on the interval $(-1,1)$, otherwise, the coefficients will be unrestricted. Beware that, under no restriction on the coefficients, there is no guarantee that the estimated coefficients will imply in a stationary process, furthermore, if the order of the AR block is greater than 1, then the restriction imposed when `AR.support` is equal to `"constrained"` does **NOT** guarantee that the process will be stationary, as such, the user is not allowed to use constrained parameters when the order of the block is greater than $1$. To constrain $\phi_t$ to the interval $(-1,1)$, we apply the inverse Fisher transformation, also known as the hyperbolic tangent function.

The pulse matrix $X$ is informed through the argument `pulse`, with the dimension of $\rho_t$ being implied by the number of columns in $X$. It is important to notice that the package expects that $X$ will inform the pulse value for each time instance, interpreting each column as a distinct pulse with an associated coordinate of $\rho_t$.

Finally, we can summarize the usage of the `AR_block` function as follows:

- `a1`, `R1` are the parameter for the prior for the AR coefficient $(\theta_1,...,\theta_{1-order})'$;
- `noise.var`, `noise.disc` and `h` define the mean and variance of random fluctuations of $\theta_t$ through time;
- `a1.coef`, `R1.coef` are the parameter for the prior for the coefficients $\phi_1, ...,\phi_{order}$;
- `h.coef`, `H.coef` and `D.coef` define the mean and variance of random fluctuations of $\phi_t$ through time;
- `a1.pulse`, `R1.pulse` are the parameter for the prior for the pulse coefficient $\rho_1$;
- `h.pulse`, `H.pulse` and `D.pulse` define the mean and variance of random fluctuations of $\rho_t$ through time; 
- `pulse` is the pulse matrix $X$;
- `AR.support` defines the support for the AR coefficients $\phi_t$.

Bellow we present the code for a simply $AR(1)$ block with $W_t=0.1, \forall t$: 

```{r eval=FALSE, include=TRUE}
mean_block <- AR_block(
  eta = 1,
  order = 1,
  noise.var = 0.1
)
```

Finally we present a plot of a Gamma model with known shape $\alpha=1.5$ and a AR structure for the mean fitted with simulated data. We will refrain to show the code for fitting the model itself, since we will discuss the tools for fitting in a section of its own.

```{r echo=FALSE, fig.height=9, fig.width=16}
T <- 200
phi <- 0.95
sigma2 <- 2
ht <- rep(NA, T)
ht_i <- 0
for (i in 1:T) {
  ht_i <- phi * ht_i + rnorm(1, 0, sqrt(sigma2))
  ht[i] <- ht_i
}
# plot(exp(ht))

data <- rgamma(T, 3 / 2, (3 / 2) / exp(ht))

volatility <- AR_block(
  eta = 1, order = 1,
  noise.var = sigma2
)

##########
fitted.data <- fit_model(
  volatility,
  Gamma(phi = 3 / 2, mu = "eta", data = data)
)

# show_fit(fitted.data, lag=-1)$plot
par(mfrow = c(1, 2))
x <- seq(fitted.data$mts[2, T] - 4 * sqrt(fitted.data$Cts[2, 2, T]),
  fitted.data$mts[2, T] + 4 * sqrt(fitted.data$Cts[2, 2, T]),
  l = 1000
)
fx <- dnorm(x, fitted.data$mts[2, T], sqrt(fitted.data$Cts[2, 2, T]))
plot(x, fx, main = "Posterior distribuition for the AR coefficient", type = "l", xlab = 'phi', ylab = "Density")
lines(c(0.95, 0.95), c(0, max(fx) + 1), lty = 2)
legend("topright", legend = c("True phi"), lty = c(2))

plot(ht, main = "Latent states estimation", xlab = "Time", ylab = "theta_t")
lines(fitted.data$mts[1, ])
lines(fitted.data$mts[1, ] - 1.96 * sqrt(fitted.data$Cts[1, 1, ]), lty = 2)
lines(fitted.data$mts[1, ] + 1.96 * sqrt(fitted.data$Cts[1, 1, ]), lty = 2)
legend("bottomleft", legend = c("True states", "Estimated states"), lty = c(0, 1), pch = c(1, NA))
par(mfrow = c(1, 1))
```

### Some comments about autoregressive models in the Normal family

The user may have notice that the autoregressive block described above is a little different from what is most common in the literature. Specifically, we do not assume that the observed data itself ($Y_t$) follows an autoregressive evolution, but instead $\theta_t$ does. This approach is a generalization of the usual autoregressive model, indeed, if we have that $Y_t$ follows an usual AR(k), such that:

$$
\begin{align}
Y_t&=\sum_{i=1}^{k}\phi_{i,t}Y_{t-1}+\epsilon_t,\\
\epsilon_t &\sim \mathcal{N}_1(0,\sigma_t^2),
\end{align}
$$
then, this model can also be written as:

$$
\begin{align}
Y_t|\eta_t&\sim \mathcal{N}_1(\eta_t,0),\\
\eta_t=\theta_t&=\sum_{i=1}^{k}\phi_{i,t}\theta_{t-i}+\omega_t,\\
\omega_t &\sim \mathcal{N}_1(0,W_t),
\end{align}
$$
such that this model can be described using the `AR_block` function.

More generally, if we have that $Y_t|\eta_t \sim \mathcal{F}(\eta_t)$, where $\mathcal{F}$ is a distribution family contained in the exponential family and indexed by $\eta_t$, then we have that:

$$
\begin{align}
Y_t|\eta_t &\sim \mathcal{F}(\eta_t),\\
g(\eta_t)=\theta_t&=\sum_{i=1}^{k}\phi_{i,t}\theta_{t-i}+\omega_t,\\
\omega_t &\sim \mathcal{N}_1(0,W_t).
\end{align}
$$

It is important to note that there is some caveats about the first specification (the usual one) and the more general one presented above. As the reader will see further bellow, we offer, as a particular case, the Normal distribution with both unknown mean and observational variance, where we can specify predictive strucutre for **both** the mean and the observational variance. In this model, it does matter if the evolution error is associated with the observation equation or the evolution equation (we cannot specify predictive structure for former, but to the latter we can). For such cases, we recommend the use of the `regression_block` function instead of the `AR_block`.

Here we present an example of the specification of an AR(k) using the `regression_block` function for a time series $Y_t$ of length $T$:

```{r eval=FALSE}
regression_block(
  mu=c(0,Y[-T]),
  max.lag=k
)
```

In the Advanced Examples section we will provide a wide range of examples, including ones with the aforementioned structures. In particular, we will present the code for some usual (yet different from what we discussed) forms of AR, including the following model:

$$
\begin{align}
Y_t&=\mu_t+\sum_{i=1}^{k}\phi_{i,t}(Y_{t-1}-\mu_{t-1})+\epsilon_t,\\
\epsilon_t &\sim \mathcal{N}_1(0,\sigma_t^2),
\end{align}
$$

## A structure for overdispersed models

```{r eval=FALSE, include=TRUE}
noise_block(..., name = "Noise", D = 0.99, R1 = 1)
```

This function will creates a sequence of **independent** latent variables $\epsilon_1,...,\epsilon_t$ such that:

$$
\begin{align}
\epsilon_{t} &\sim \mathcal{N}(0,\sigma_t^2),\\
\sigma_t^2&=\frac{t-1}{t}D_t\sigma_{t-1}^2+\frac{1}{t}(1-D_t)\mathbb{E}[\epsilon_{t-1}^2|\mathcal{D}_{t-1}],\\
\sigma_1^2&=R_1.
\end{align}
$$

Notice that the user do not need to specify the matrix $G_t$, since it is implicitly determined by the equations above, such that $G_t=0$ for all $t$.

It is easy to see the correspondence between most of the arguments of the `noise_block` function and their respective meaning in the block specification, while the remaining ones follow the same usage seen in the previous block functions (see the `polynomial_block` function). Here we will only emphasize the usage of the `...` argument.

The argument `...` is used to specify the matrix $F_t$, specifically, the user must provide a list of named values, whose name indicate a linear predictor $\lambda_t$ and its associated value represent the effect of $\epsilon_{t}$ in this predictor. Naturally, if a linear predictor is not present in `...`, it is understood that $\epsilon_{t}$ have no effect whatsoever on that particular linear predictor. Such specification of $F_t$ may seem strange, but as the reader will see further bellow, this way of specifying $F_t$ is very useful to avoid confusion when dealing with multiple linear predictors.

It is important to emphasize that the dimension of $\lambda_t$ is implicitly determined by `...`, since one linear predictor will be created for each **unique** named value passed by the user.

As the user must have noticed, this block makes no sense on its own, since it has barely any capability of learning patterns. But, we is shown in the next subsection, structural blocks can be combined with each other, such that the noise block would be only one of several other structural blocks in a model.

To exemplify the utility of this structural block, let us assume we want to model the following (simulated) time series of counts:

```{r echo=FALSE}
set.seed(13031998)
T=200
mu=20*(1:T)*(T:1)/(T**2)
data <- rpois(T, exp(mu+rnorm(T,0,sqrt(0.1))))
ts.plot(data)
```

Since the data is a counting, its natural to propose a Poisson model, such that:

$$
\begin{align}
Y_t|\theta_t &\sim Poisson\left(\eta_t\right),\\
\ln(\eta_t) &=\lambda_{t}=\theta_t,\\
\theta_t&=\theta_{t-1}+\omega_t,\\
\omega_t &\sim \mathcal{N}_1(0,W_t),
\end{align}
$$

Bellow we present that model fitted using the `kDGLM` package:

```{r, results='hide'}

level <- polynomial_block(
  rate = 1,
  order=3,
  D = 0.95
)

fitted.data <- fit_model(level,
                         'Model 1'=Poisson(lambda="rate",data=data)
)

plot(fitted.data, lag = 1, plot.pkg = "base")
```

Notice that the data at the middle of the observed period is overdispersed, such that a Poisson model cannot properly address the uncertainty. One could proposed the usage of a Normal model which, indeed, could capture the uncertainty in the middle, but notice that the data at the beginning and at the end of the series has very low values, such that a Normal model would be inappropriate. In such scenario, a better approach would be to add an noise component to the linear predictor, such that it can capture the overdispersion:

```{r, results='hide'}

level <- polynomial_block(
  mu = 1,
  order=3,
  D = 0.95
)
noise <- noise_block(
  mu = 1
)

fitted.data <- fit_model(level, noise,
                         'Model 2'=Poisson(lambda="mu",data=data)
)

plot(fitted.data, lag = 1, plot.pkg = "base")
```

It is relevant to point out that the choice of `R1` can affect the final fit, as such, we highly recommend the user to perform a sensibility analysis to help specify the value of `R1`.

Lastly, as we will see latter on, the noise block can also be useful to model the dependency between multiple time series.


## Handling multiple structural blocks

In the previous subsections, we discussed how to define the structure of a model using the functions `polynomial_block`, `regression_block`, `harmonic_block`, `AR_block` and `noise_block`, yet we have dealt mostly with cases where a model had only one of those structures. Generally, the user will want to mix and match multiple types of structures, each one being responsible to explain part of the outcome $Y_t$ (as seen in the case of the `noise_block` function). For this task, we introduce two operator: one to combine (superposition) and other to multiple blocks.

Suppose that we have a model with a variety of structural blocks such that:

$$
\begin{align}
\theta_t&=\begin{bmatrix}\theta_t^1\\ \vdots\\ \theta_t^n\end{bmatrix}\\
F_t&=\begin{bmatrix}F_t^1 & \dots & F_t^n\end{bmatrix}\\
G_t&=diag\{G_t^{1},...,G_t^{n}\},\\
W_t&=diag\{W_t^{1},...,W_t^{n}\},
\end{align}
$$
where $diag\{M^1,...,M^{n}\}$ represents a block diagonal matrix such that its diagonal is composed of  $M^1,...,M^{n}$; $\theta_t$ is the vector obtained by the concatenation of the vectors $\theta_t^1,..., \theta_t^n$; and $F_t$ is a matrix obtained by the column-wise concatenation of the matrices $F_t^1,..., F_t^n$ (remember that each line of $F_t$ correspond to one linear predictor, such that $F_t^1,..., F_t^n$ all have the same number of lines).

In this scenario, to facilitate the specification of such model, we could create one structural block for each $\theta_t^i$, $F_t^{i}$, $G_t^{i}$ and $W_t^{i}$, $i=1,...n$, and then "combine" all blocks together. This operation is called *superposition* and can be found described in details in @WestHarr-DLM, section 6.2. Our package allows that operation through the function `block_superpos` or, (almost always) equivalently, through the `+` operator:

```{r eval=FALSE, include=FALSE}
block_1 <- ...
.
.
.
block_n <- ...

complete_structure <- block_superpos(block_1, ..., block_n)
# or
complete_structure <- block_1 + ... + block_n
```

To demonstrate the usage of this operator, suppose we would like to create a model using four of the structures presented in the previous sections (a polynomial trend, an dynamic regression, a harmonic trend and an AR model). We could do so with the following code:

```{r eval=FALSE, include=TRUE}
poly_subblock <- polynomial_block(
  eta = 1,
  order = 1,
  name = "Poly",
  D = 0.95
)

regr_subblock <- regression_block(
  eta = X,
  order = 12,
  name = "Regr",
  D = 0.95
)

harm_subblock <- harmonic_block(
  eta = 1,
  period = 12,
  name = "Harm",
  D = 0.975
)

AR_subblock <- AR_block(
  eta = 1,
  order = 1,
  noise.var = 0.1,
  name = "AR"
)

complete_block <- poly_subblock + regr_subblock + harm_subblock + AR_subblock
```

Ideally, the user should provide each block with a name to help identify them after the model is fitted, but, if the user does not provide a name, the block will have the default name for that type of block. In case one or more blocks have the same name, the package will automatically add an index to the variables with conflicting names based on the order that the blocks were combined. Note that the automatic naming might make the analysis of the fitted model confusing, specially when dealing with a large number of latent variable, with that in mind, we **strongly** recommend the users to specify an intuitive name for each structural block.

Lastly, we present the `block_mult` function and the associated operator `*`. This function allows the user to create multiple blocks with identical structure, but each one being associated with a different linear predictor. The usage of this function is as simple as:

```{r echo=TRUE}
base.block <- polynomial_block(
  eta = 1,
  order = 1,
  name = "Poly",
  D = 0.95
)

final.block <- block_mult(base.block, 4)
# or
final.block <- 4 * base.block
# or
final.block <- base.block * 4
```

When multiplying blocks, the package understands that each copy of the base block is independent of each other (i.e., they have their own latent states) and each block is associated with a different set of linear predictors. The name of the linear predictors associated with each block are taken to be the original name with a index:

```{r echo=TRUE}
final.block <- 4 * base.block
final.block$pred.names
```


Naturally, the user might want to rename the linear predictors to a more intuitive label. For such task, we provide the `rename_block` function, whose usage is as follows:

```{r echo=TRUE}
final.block <- block_rename(final.block, c("Matthew", "Mark", "Luke", "John"))
final.block$pred.names
```

## Handling multiple linear predictos

As the user may have noticed, more then one argument can be passed in the  `...` argument, indeed, if the user does so, the package will create multiple linear predictors in the same block (one for each unique name), all of which are affected by the associated latent state. For instance, take the following code:

```{r eval=FALSE}
polynomial_block(lambda1=1,lambda2=1,lambda3=1)
```

The code above creates $3$ linear predictors, such that:

$$
\lambda_{1,t}=1 \times \theta_{t}\\
\lambda_{2,t}=1 \times \theta_{t}\\
\lambda_{3,t}=1 \times \theta_{t}\\
$$

Naturally, in this simple case all linear predictors are identical, but it does not need to be so, for instance:

```{r eval=FALSE}
polynomial_block(lambda1=1,lambda2=5,lambda3=X) # Assuming there is an variable X in the environment
```

This code will create $3$ linear predictors, such that:

$$
\begin{align}
\lambda_{1,t}&=1 \times \theta_{t}\\
\lambda_{2,t}&=5 \times \theta_{t}\\
\lambda_{3,t}&=X_t \times \theta_{t}\\
\end{align}
$$

Also, one can use multiple blocks in the same structure to define linear predictors that share some (but not all) of their components:

```{r eval=FALSE}
polynomial_block(lambda1=1)+ # theta_1
  polynomial_block(lambda2=1)+ # theta_2
  polynomial_block(lambda3=1)+ # theta_3
  polynomial_block(lambda1=1,lambda2=1,lambda3=1) # theta_4
```

Such that:

$$
\begin{align}
\lambda_{1,t}&=\theta_{1,t}+\theta_{4,t}\\
\lambda_{2,t}&=\theta_{2,t}+\theta_{4,t}\\
\lambda_{3,t}&=\theta_{3,t}+\theta_{4,t}\\
\end{align}
$$

And also, the user may specify unknown components in the matrix $F_t$, such as to estimate the impact of some of the shared components:

```{r eval=FALSE}
polynomial_block(lambda1=1,lambda3='lambda2')+ # theta_1
  polynomial_block(lambda2=1)+ # theta_2
  polynomial_block(lambda3=1) # theta_3
```

$$
\begin{align}
\lambda_{1,t}&=\theta_{1,t}\\
\lambda_{2,t}&=\theta_{2,t}\\
\lambda_{3,t}&=\theta_{3,t}+\lambda_{2,t}\theta_{1,t}=\theta_{2,t}\theta_{1,t}\\
\end{align}
$$

More details about this last example are presented in the following subsection.

## Handling unknown components in the planning matrix $F_t$

In some situations the user may want to fit a model such that:

$$
\require{color}
\begin{align}
\lambda_{t}=F_t'\theta_t=\cdots+\phi_t\theta_t +\cdots,
\end{align}
$$
in other words, it may be the case that the planning matrix $F_t$ contains one or more unknown components. This idea may be foreign when working with only one linear predictor, but if our observational model has several parameters it could make sense to have shared effects between parameters. Beside that, this construction is also natural when modeling multiple time series simultaneously, such as when dealing with correlated outcomes or when working with a compound regression. All those cases will be explored in the Advanced Examples section. For now, we will focus on **how** to specify such models, whatever they use may be.

For simplicity, let us assume that we want to create a linear preditor $\lambda_t$ such that $\lambda_{t}=\phi_t\theta_t$. Then the first step would be to create a linear predictor associated with $\phi_t$ (which we will call `phi`, altough the user may call it whatever it pleases him):

```{r}
phi_block=polynomial_block(phi=1)
```

Notice that we are creating a linear predictor $\phi_t$ and a latent variable $\tilde{\theta}_t$ such that $\phi_t=1\times \tilde{\theta}_t$. Also, it is important to note that the structure for $\phi_t$ could be any of the other structural blocks (harmonic, regression, autoregression, etc.).

Now we can create a structural block for $\theta_t$:

```{r}
theta_block=polynomial_block(lambda='phi')
```


The code above creates a linear predictor $\lambda_t$ and a latent state $\theta_t$ such that $\lambda_t=\phi_t \times \theta_t$. Notice that the `...` argument of any structural block is used to specify the planning matrix $F_t$, specifically, the user must provide a list of named values, whose name indicate a linear predictor $\lambda_t$ and its associated value represent the effect of $\theta_{t}$ in this predictor. When the user pass a string in `...`, the package understand that the component of $F_t$ associated with $\theta_t$ is unknown and is being modeled by the linear predictor whose name is the passed string.

Lastly, as one could guess, it is possible to create a chain of components in $F_t$ such as to create an even more complex structure. For instance, take the code bellow:

```{r eval=FALSE}
polynomial_block(eta1=1)+
  polynomial_block(eta2='eta1')+
  polynomial_block(eta3='eta2')
```

In the first line we create a linear predictor $\eta_{1,t}$ such that $\eta_{1,t}=1 \times \theta_{1,t}$. In second line we create another linear predictor $\eta_{2,t}$ such that $\eta_{2,t}=\eta_{1,t} \times \theta_{2,t}=\theta_{1,t} \times \theta_{2,t}$. Then we create a linear predictor $\eta_{3,t}$ such that $\eta_{3,t}=\eta_{2,t} \times \theta_{3,t}=\theta_{1,t} \times \theta_{2,t} \times \theta_{3,t}$.

# Creating the model outcome

Until now, we have presented the tools for creating the structure of a DGLM model, specifically, we have shown how to define the relationship between the latent vector $\theta_t$ and the linear predictors $\lambda_t$, along with the temporal dynamic of $\theta_t$. Now we proceed to define the observational model for $Y_t$ and the relationship between $\lambda_t$ and $\eta_t$, the parameters for the observational model, i.e., the highlighted part of the following equations:

$$
\require{color}
\begin{align}
\color{red}{Y_t|\eta_t }&{\color{red}\sim \mathcal{F}\left(\eta_t\right),}\\
{\color{red}g(\eta_t) }&{\color{red}= \lambda_{t}}=F_t'\theta_t,\\
\theta_t &=G_t\theta_{t-1}+\omega_t,\\
\omega_t &\sim \mathcal{N}_n(h_t,W_t),
\end{align}
$$

In each subsection, we will assume that the linear predictors are already defined, along with all the structure that must come along with them (i.e., we will take for granted the part that is not highlighted), moreover, we also assume that the user have created the necessary amount of linear predictors for each type of outcome (we will see, in each case, what that amount is) and that those linear predictors were named as $\lambda_1$,...,$\lambda_k$.

## Normal case

In some sense, we can think of this as the most basic case, at least in a theoretical point of view, since the Kalman Filter was first developed for this specific scenario. Indeed, if we have a **static** observational variance/covariance matrix (even if unknown), we fall within the DLM class, which has an exact analytical solution for the posterior of the latent states. With some adaptations, one can also have some degree of temporal dynamic for the variance/covariance matrix (see @WestHarr-DLM, section 10.8). Yet, our package goes a step further, offering the possibility for predictive structure for both the mean and the observational variance/covariance matrix, allowing even the inclusion of dynamic regressions, seasonal patterns, autoregressive components, etc., for **both** parameters.

We will present this case in two contexts: the first, which is a simply implementation of the Kalman Filter and Smoother, deals with data coming from an Normal distribution (possibly multivariated) with unknown mean and known variance/covariance matrix; the second deals with data coming from a univariate Normal distribution with unknown mean and unknown variance.

Also, at the end of the second subsection, we present an extension to the bivariated Normal distribution with unknown mean and unknown covariance matrix. A study is being conducted to expand this approach to the $k$-variated case, for any arbitrary $k$.

### Normal outcome with known variance

Suppose that we have a sequence of $k$-dimensional vectors $Y_t$, such that $Y_t=(Y_{1t},...,Y_{kt})'$. We assume that:

$$
\begin{align}
Y_t|\mu_t,V &\sim \mathcal{N}_k\left(\mu_t,V\right),\\
\mu_{it}&=\lambda_{it}, i=1,...,k,\\
\end{align}
$$
where $V$ is a known symmetric, definite positive $k\times k$ matrix. Also, for this model, we assume that the link function $g$ is the identity function.

To create an outcome for this model, we can make use of the `Normal` function:

```{r eval=FALSE, include=TRUE}
Normal(
  mu,
  V = NA,
  Tau = NA,
  Sigma = NA,
  Sd = NA,
  data
)
```

Intuitively, the `mu` argument must be a character vector of size $k$ containing the names of the linear predictors associated with each $\mu_{i.}$. The user must also specify one (and only one) of `V`, `Tau`, `Sigma` or `Sd`. If the user provides `V` or `Sigma`, $V$ is assumed to be that value; if the user provides `Tau`, $V$ is assumed to be the inverse of the given matrix (i.e., `Tau` is the precision matrix); if the user provides `Sd`, $V$ is assumed to be such that the standard deviation of the observations is equal to the main diagonal of `Sd` and the correlation between observations is assumed the be equal to the off-diagonal elements of `Sd`. 


```{r eval=FALSE, include=FALSE}
# As the user have seen in previous examples, the package accepts that the parameter names may be passed as a string, but **only** when there is no variable with the same name in the scope. If such variable exists, the **value** of that variable will be used instead of its name. In general, we advise
```

The `data` argument must be a $T \times k$ matrix containing the values of $Y_t$ for each observation. Notice that each line $i$ must be the values of all categories in time $i$ and each column $j$ must represent the values of a category $j$ through time.

Next, we present a brief example for the usage of this outcome for a univariate outcome (the general case works similarly). We use some functions described in the previous sections, as well as some functions that will be presented later on, for now, let us focus only on the usage of the `Normal` function.

```{r, results='hide'}
# Univariate Normal case

data <- c(log(AirPassengers))

level <- polynomial_block(mu = 1, D = 0.95, order = 2)
season <- harmonic_block(mu = 1, period = 12, D = 0.975)

outcome <- Normal(mu = "mu", V = 6e-3, data = data)
fitted.data <- fit_model(level,season, outcome)
plot(fitted.data, plot.pkg = "base")
```

Notice that, since this is the univariate case, the `data` argument can be a vector.

### Univariated Normal outcome with unknown variance

For this type of outcome, we assume that:

$$
\begin{align}
Y_t|\mu_t,\tau_t &\sim \mathcal{N}\left(\mu_t,\tau_t^{-1}\right),\\
\mu_{t}&=\lambda_{1t},\\
\ln\{\tau_{t}\}&=\lambda_{2t}.\\
\end{align}
$$

To create an outcome for this model, we also make use of the `Normal` function:

```{r eval=FALSE, include=TRUE}
Normal(
  mu,
  V = NA,
  Tau = NA,
  Sigma = NA,
  Sd = NA,
  data
)
```

Just as before, the `mu` argument must be a character representing the name of the linear predictor associated with $\mu_t$. The user must also specify one (and only one) of `V`, `Tau`, `Sigma` or `Sd`, which must be a character string representing the name of the linear predictor associated with $\tau_t^{-1}$, $\tau_t$, $\tau_t^{-1}$ or $\tau_t^{-1/2}$, respectively.

Similar to the known variance case, we allow multiple parametrizations observational variance. Specifically, if the user provides `V` or `Sigma`, we assume that $\lambda_{2t}=\ln\{\sigma^2_{t}\}=-\ln\{\tau_t\}$; if the user provides `Sd`, we assume that $\lambda_{2t}=\ln\{\sigma_{t}\}=-\ln\{\tau_t\}/2$; if the user provides `Tau`, then the default parametrization is used, i.e., $\lambda_{2t}=\ln\{\tau_t\}$. 

The `data` argument must be a $T \times 1$ matrix containing the values of $Y_t$ for each observation. If `data` is a vector, we assume that $Y_t$ is univariated and each coordinate of `data` represents the observed value at each time. If a value of data is not available (`NA`) for a specific time, the package will consider that there was no observation at that time, such that the update step of the filtering algorithm will be skipped.

Next, we present a brief example for the usage of this outcome:

```{r, results='hide'}
# Univariate Normal case
structure <- polynomial_block(mu = 1, D = 0.95) +
  polynomial_block(V = 1, D = 0.95)

outcome <- Normal(mu = "mu", V = "V", data = cornWheat$corn.log.return[1:500])
fitted.data <- fit_model(structure, outcome)
plot(fitted.data, plot.pkg = "base")
```

Currently, we also support models with bivariate Normal outcomes. In this scenario we assume the following model:

$$
\begin{align}
Y_t|\mu_{t},V_t &\sim \mathcal{N}_2\left(\mu_t,V_t\right),\\
\mu_t&=\begin{bmatrix}\mu_{1t}\\ \mu_{1t}\end{bmatrix},\\
V_t&=\begin{bmatrix}\tau_{1t}^{-1} & \sqrt{\tau_{1t}\tau_{2t}}\rho\\ \sqrt{\tau_{1t}\tau_{2t}}\rho & \tau_2^{-1}\end{bmatrix},\\
\mu_{it}&=\lambda_{it}, i=1,2\\
\tau_{it}&=\ln\{\lambda_{(i+2)t}\}, i=1,2\\
\rho_{t}&=\tanh\{\lambda_{5t}\}.\\
\end{align}
$$

Notice that $\rho_t$ represents the **correlation** (and **not** the covariance) between the series at time $t$ and, to guarantee that $\rho_t \in (-1,1)$, we use the Inverse Fisher transformation (also known as the hyperbolic tangent function) as link function.

For those models, `mu` must be a character vector, similarly to the case where $V$ is known, and `V`, `Tau`, `Sigma` and `Sd` must be a $2 \times 2$ character matrix. The diagonal elements are interpreted as the linear predictors associated with the precisions, variances or standard deviations, respectively, while the off diagonal elements must be equals (one of then can be equal to `NA`) and will be interpreted as the linear predictor associated with $\rho_t$.

Bellow we present an example for the bivariate case. We use some functions described in the previous sections, as well as some functions that will be presented later on, for now, let us focus only on the usage of the `Normal` function.

<div>
#### {.tabset}
```{r, results='hide'}
# Bivariate Normal case
structure <- (polynomial_block(mu = 1, D = 0.95) +
  polynomial_block(log.V = 1, D = 0.95)) * 2 +
  polynomial_block(atanh.rho = 1, D = 0.95)

outcome <- Normal(
  mu = c("mu.1", "mu.2"),
  V = matrix(c("log.V.1", "atanh.rho", "atanh.rho", "log.V.2"), 2, 2),
  data = cornWheat[1:500, c(4, 5)]
)
fitted.data <- fit_model(structure, outcome)
```

##### Predictions

```{r, results='hide'}
plot(fitted.data, plot.pkg = "base")
```

##### Correlation

```{r, results='hide'}
plot_lin_pred(fitted.data,'atanh.rho', plot.pkg = "base")
```

</div>

Notice that, by the second plot, the correlation between the series (represented by `atanh.rho`, i.e., the plot shows $\tanh^{-1}(\rho)$) is significant and changes over time, making the proposed model much more adequate than two independent Normal models (one for each outcome).

## Poisson case

In this case, we assume the following observational model:

$$
\begin{align}
Y_t|\eta_t &\sim Poisson\left(\eta_t\right),\\
\ln(\eta_t) &=\lambda_{t}.
\end{align}
$$

In the notation introduced before, we have that our link function $g$ is the (natural) logarithm function.

To define such observational model, we offer the `Poisson` function, whose usage is presented bellow:

```{r eval=FALSE, include=TRUE}
Poisson(
  lambda,
  data,
  offset = data^0
)
```

As usual in the literature, we refer to the rate parameter of the Poisson distribution as `lambda` (although, in the context of this document, this might seem confusing) and the user must provide for this argument the name of the linear predictor associated with this parameter.

For the argument `data` the user must provide a sequence of numerical values consisting of the observed values of $Y_t$ at each time. Since the $Y_t$ is a scalar for all $t$, the user can pass the outcome as a vector or as a matrix with a single column.  If a value of data is not available (`NA`) for a specific time, the package will consider that there was no observation at that time, such that the update step of the filtering algorithm will be skipped.

Lastly, the `offset` argument is optional and can be used to provide a measure of the scale of the data. If the offset is provided and is equal to $E_t$, then we will fit a model assuming that:

$$
\begin{align}
Y_t|\theta_t &\sim Poisson\left(\eta_tE_t\right),\\
\ln(\eta_t) &=\lambda_{t}.
\end{align}
$$

Bellow we present an example of the usage of this outcome. We use some functions described in the previous section, as well as some functions that will present later on, for now, let us focus only on the usage of the `Poisson` function.

```{r, results='hide'}
data <- c(AirPassengers)

level <- polynomial_block(rate = 1, order = 2, D = 0.95)
season <- harmonic_block(rate = 1, period = 12, order=2, D = 0.975)

outcome <- Poisson(lambda = "rate", data = data)

fitted.data <- fit_model(level, season,
  AirPassengers = outcome
)
plot(fitted.data, plot.pkg = "base")
```

Notice that, while creating the structure, we defined a linear predictor named `rate`, whose behavior is being explained by a second order polynomial trend and seasonal component defined by a second order harmonic block. Since the value passed to `rate` equals $1$ in both blocks, we have that these components have a constant effect (and equal to $1$) on the linear predictor on all times, although **the components themselves change their values over time** such as to capture the behavior of the series.

Later on, when creating the outcome, we pass the name `'rate'` as the linear predictor associated with `lambda`, the rate (or mean) parameter of the Poisson distribution.

This is a particularly simply usage of the package, the Poisson kernel being the one with the smallest amount of parameters. Moving forward, we will present outcomes whose specification can be a bit more complex.

## Gamma case

In this subsection we will present the Gamma case, in which we assume the following observational model:

$$
\begin{align}
Y_t|\alpha_t,\beta_t &\sim \mathcal{G}\left(\alpha_t,\beta_t\right),\\
\ln\{\alpha_t\}&=\lambda_{1t},\\
\ln\{\beta_t\}&=\lambda_{2t}
\end{align}
$$

For this outcome we have a few variations. First, there's a matter of parametrization. We allow the user to define the model by any non redundant pair of:

$$
\begin{align}
\alpha_t&,\\
\beta_t&,\\
\phi_t&=\alpha_t,\\
\mu_t&=\frac{\alpha_t}{\beta_t},\\
\sigma_t&=\frac{1}{\beta_t}.
\end{align}
$$

Naturally, the user **CANNOT** specify both $\alpha_t$ AND $\phi_t$ or $\beta_t$ AND $\sigma_t$, as such specification is redundant at best, and incoherent at worst. Outside of those cases, in which the package will raise an error, any combination can be used by the user, allowing for the structure of the model to be defined within the variables that are most convenient (it may be easier or more intuitive to specify the structure in the mean $\mu_t$ and the scale $\sigma_t$, than on the shape $\alpha_t$ and rate $\beta_t$).

Another particularity of the Gamma outcome is that the user may set the shape parameter $\phi_t$ to a known constant. In that case, the user must specify the structure to the mean parameter $\mu_t$ (he is not allowed to specify neither $\beta_t$ nor $\sigma_t$). In general, we do not expect the shape parameter to be known, still, there are some important applications where it is common the use some particular cases of the Gamma distribution, such as the Exponential Model ($\phi_t=1$) or the $\chi^2$ model ($\phi_t=0.5$). The estimation of the shape parameter $\phi_t$ is still under development, as such, the current version of the package does not have support for a unknown $\phi_t$ (a version of the package with a proper estimation for $\phi_t$ will be released very soon).

No matter the parametrization, the link function $g$ will always be the logarithm function, as such, given a certain parametrization, we can write the linear predictor of any other parametrization as a linear transformation of the original.

In the examples of this section, we will always use the parameters $\phi_t$ (when applicable) and $\mu_t$, but the code used can be trivially adapted to other parametrizations.

```{r eval=FALSE, include=TRUE}
Gamma(
  phi = NA,
  mu = NA,
  alpha = NA,
  beta = NA,
  sigma = NA,
  data = ,
  offset = data^0
)
```

Similar to the Poisson case, the argument `data` must provide a set of numerical values consisting of the observed values of $Y_t$ at each time. Since the $Y_t$ is a scalar for all $t$, the user can pass the outcome either as a vector or as a matrix with a single column. If a value of data is not available (`NA`) for a specific time, the package will consider that there was no observation at that time, such that the update step of the filtering algorithm will be skipped.

The `offset` argument is optional and can be used to provide a measure of the scale of the data. If the offset is provided and is equal to $E_t$, then we will fit a model assuming that:

$$
\begin{align}
Y_t|\theta_t &\sim \mathcal{G}\left(\alpha_t,\beta_t E_t^{-1}\right).
\end{align}
$$

Note that the above model implies that:

$$
\mathbb{E}[Y_t|\theta_t]=\frac{\alpha_t}{\beta_t}E_t.
$$

The arguments `phi`, `mu`, `alpha`, `beta` and `sigma` should be character strings indicating the name of the linear predictor associated with their respective linear predictor. The user may opt to pass `phi` as a positive numerical value, it that case, the shape parameter $\phi_t$ is considered known and equal to `phi` for all $t$.

```{r, results='hide'}

structure <- polynomial_block(mu = 1, D = 0.95)

outcome <- Gamma(phi = 0.5, mu = "mu", data = cornWheat$corn.log.return[1:500]**2)
fitted.data <- fit_model(structure, outcome)
plot(fitted.data, plot.pkg = "base")

```

## Multinomial case

Let us assume that we have a sequence of $k$-dimensional non-negative integer vectors $Y_t$, such that $Y_t=(Y_{1t},...,Y_{kt})'$ and:

$$
\begin{align}
Y_t|N_t,\vec{p}_t &\sim Multinom\left(N_t,\vec{p}_t\right),\\
\ln\left\{\frac{p_{it}}{p_{kt}}\right\}&=\lambda_{it}, i=1,...,k-1,\\
N_t&=\sum_{i=1}^{k}Y_{it},
\end{align}
$$
where $\vec{p}_t=(p_{1t},...,p_{kt})'$, with $p_{it} > 0, \forall i$ and $\sum_{i=1}^k p_{it}=1$.

Notice that $N_t$ is automatically defined by the values of $Y_t$, such that $N_t$ is always considered a known parameter. Also, it is important to point out that this model has only $k-1$ free parameters (instead of $k$), since the restriction $\sum_{i=1}^k p_{it}=1$ implies that defining $k-1$ entries of $\vec{p}_t$ defines the remaining value. Specifically, we will always take the last entry (or category) of $Y_t$ as the reference value, such that $p_{kt}$ can be considered as the baseline probability of observing data from a category (i.e., we will model how each $p_{it}$ relates to the baseline probability $p_{kt}$).

To create an outcome for this model, we can make use of the `Multinom` function:

```{r eval=FALSE, include=TRUE}
Multinom(
  p,
  data,
  offset = data^0
)
```

For the Multinomial case, `p` must be a character **vector** of size $k-1$ containing the names of the linear predictors associated with $\ln\left\{\frac{p_{it}}{p_{kt}}\right\}$ for each $i=1,...,k-1$.

The `data` argument must be a $T \times k$ matrix containing the values of $Y_t$ for each observation. Notice that each line $i$ must represent the values of all categories in time $i$ and each column $j$ must represent the values of a category $j$ through time. If a value of data is not available (`NA`) for a specific time, the package will consider that there was no observation at that time, such that the update step of the filtering algorithm will be skipped.

The `offset` argument is optional and must have the same dimensions of `data` (its dimensions are interpreted in the same manner). The argument can be used to provide a measure of the scale of the data and, if the offset is provided, such that, at each time $t$, the offset is equal to $E_t=(E_{1t},...,E_{kt})'$, then we will fit a model assuming that:

$$
\begin{align}
Y_t|\theta_t &\sim Multinom\left(N_t,\vec{p}^*_t\right),\\
\ln\left\{\frac{p^*_{it}}{p^*_{kt}}\right\}&=\ln\left\{\frac{p_{it}}{p_{kt}}\right\}+\ln\left\{\frac{E_{it}}{E_{kt}}\right\}, i=1,...,k-1.
\end{align}
$$

At the end of this subsection we present a brief discussion about the implications of the inclusion of the offset and how to interpret it, as well as a explanation for the way we chose to include it.

Again, we present a brief example for the usage of this outcome:

```{r, results='hide'}
structure <- (
  polynomial_block(p = 1, order = 2, D = 0.95) +
    harmonic_block(p = 1, period = 12, D = 0.975) +
    noise_block(p=1)+
    regression_block(p = chickenPox$date >= as.Date('2013-09-1')) # Vaccine was introduced in September of 2013
) * 4

outcome <- Multinom(p = structure$pred.names, data = chickenPox[, c(2, 3, 4, 6, 5)])
fitted.data <- fit_model(structure, outcome)
plot(fitted.data, plot.pkg = "base")
```

### Some comments on the usage of an offset

The model presented in this section is intend to describe a phenomena such that we have $N_t$ subjects that were distributed randomly (but not necessarily uniformly randomly) among $k$ categories. In this scenario, $p_{it}$ represent the probability of one observation to fall within the category $i$, such that:

$$
p_{it}=\mathbb{P}(Y_{it}=1|N_t=1).
$$

In some applications, it might be the case that $N_t$ represents the counting of some event of interest and we want to model the probability of this event occurring in each category. In this scenario, it is not clear how to use the multinomial model, since we will have that:

$$
p_{it}=\mathbb{P}(\text{Observation belong to category }i|\text{Event occured}),
$$
but we actually want to known:

$$
p^*_{it}=\mathbb{P}(\text{Event occured}|\text{Observation belong to category }i).
$$

Notice that we can write:

$$
\begin{aligned}
p^*_{it}&=\mathbb{P}(\text{Event occured}|\text{Observation belong to category }i)\\
&=\frac{\mathbb{P}(\text{Observation belong to category }i|\text{Event occured})\mathbb{P}(\text{Event occured})}{\mathbb{P}(\text{Observation belong to category }i)}\\
&=\frac{p_{it}\mathbb{P}(\text{Event occured})}{\mathbb{P}(\text{Observation belong to category }i)}.
\end{aligned}
$$

The above relation implies that:

$$
\begin{aligned}
\ln\left\{\frac{p^*_{it}}{p^*_{kt}}\right\}
&=\ln\left\{\frac{p_{it}}{p_{kt}}\right\}-\ln\left\{\frac{\mathbb{P}(\text{Observation belong to category }i)}{\mathbb{P}(\text{Observation belong to category }k)}\right\}.
\end{aligned}
$$

If we pass to the `offset` argument of the `Multinom` function a set of values $E_t$, such that $E_{t} \propto (\mathbb{P}(\text{Observation belong to category }1),...,\mathbb{P}(\text{Observation belong to category }k))'$, then, by the specification provided in this section, we have that:

$$
\ln\left\{\frac{p^*_{it}}{p^*_{kt}}\right\}=\lambda_{it},
$$
in other words, the linear predictors (and consequently, the model structure) will describe the probability that an event occurs in a specific class (instead of the probability that an observation belongs to that class, given the occurrence of the event).

To obtain $p^*_{it}$ itself (i.e. the probability of the event occurring given that the observation belongs to the category $i$), one can use Bayes formula, as long $\mathbb{P}(\text{Event occured})$ is known. Indeed, one can write:

$$
\begin{aligned}
p^*_{it}&=p_{it}\frac{\mathbb{P}(\text{Event occured})}{\mathbb{P}(\text{Observation belong to category }i)}\\
        &=\frac{\exp\{\lambda_i\}}{1+\sum_j \exp\{\lambda_j\}}\frac{\mathbb{P}(\text{Event occured})}{\mathbb{P}(\text{Observation belong to category }i)}
\end{aligned}
$$

## Handling multiple outcomes

Lastly, our package also allows for the user to jointly fit multiple time series, as long as the marginal distribution of each series is one of the supported distribuitions **AND** the series are independent given the latent state vector $\theta_t$. In other words, let $\{Y_{i,t}\}_{t=1}^{T}, i =1,...,r$, be a set of time series such that:

$$
\begin{align}
Y_{i,t}|\eta_{i,t} &\sim \mathcal{F}_{i}\left(\eta_{i,t}\right),\\
g_i\left\{\eta_{i,t}\right\}&=\lambda_{i,t}=F_{i,t}'\theta_{t},
\end{align}
$$
and $Y_{1,t}, ...,Y_{r,t}$ are mutually independent given $\eta_{1,t}, ...,\eta_{r,t}$.

To fit such model, one must only pass the multiple outcomes to the `fit_model` function. As an example, we present the code for fitting two Poisson series:

```{r, results='hide'}
structure=polynomial_block(mu.1=1,mu.2=1,order=2,D=0.95)+
  harmonic_block(mu.2=1,period=12,order=2,D=0.975)+
  polynomial_block(mu.2=1,order=1,D=0.95)+
  noise_block(mu=1)*2

outcome1=Poisson(lambda='mu.1',data=chickenPox[,5])
outcome2=Poisson(lambda='mu.2',data=chickenPox[,2])

fitted.data=fit_model(structure,
                      Adults=outcome1,
                      Infants=outcome2)

plot(fitted.data,plot.pkg='base')
```

It is important to note that in our package the Multivariate Normal and the Multinomial models are **not** considered models with multiple outcomes, but instead, they are considered be one outcome each, such that the outcome itself is a vector (note that we made no restrictions on the dimension of each $Y_{i,t}$).

Also important to note is that our general approach for modeling multiple time series can not, on its own, be considered a generalization of the Multivariate Normal or Multinomial models. Specifically, if we treat each coordinate of the outcome as a outcome of its own, they would **not** satisfy the hypotheses of independence given the latent states $\theta_t$. This can be compensated with changes to the model structure, but, in general, it is better to model data using a known joint distribution than to assume conditional independence and model the outcomes dependence by shared structure (see <font color='red'> this article that I did not started to write </font>).

### Special case: Conditional modelling

There is a special type of specification for a model with multiple outcomes that does not require the outcomes to be independent given the latent states. Indeed, if the user specifies the conditional distribution of each outcome given the previous ones, then no hypotheses is needed for fitting the data. 

For instance, lets say that there are three time series $Y_{1,t},Y_{2,t}$ and $Y_{3,t}$, such that each series follows a Poisson distribution with parameter $\eta_{i,t}, i=1,2,3$. Then, $Z_t=Y_{1,t}+Y_{2,t}+Y_{3,t}$ follows a Poisson distribution with parameter $\eta_{1,t}+\eta_{2,t}+\eta_{3,t}$ and $Y_{1,t},Y_{2,t},Y_{3,t}|Z_t$ jointly follows a Multinomial distribution with parameters $N_t=Z_t$ and $\vec{p}_t=\left(\frac{\eta_{1,t}}{\eta_{1,t}+\eta_{2,t}+\eta_{3,t}},\frac{\eta_{2,t}}{\eta_{1,t}+\eta_{2,t}+\eta_{3,t}},\frac{\eta_{3,t}}{\eta_{1,t}+\eta_{2,t}+\eta_{3,t}}\right)'$. Then the user may model $Z_t$ and  $Y_{1,t},Y_{2,t},Y_{3,t}|Z_t$:

```{r, results='hide'}
structure=polynomial_block(mu=1,order=2,D=0.95)+
  harmonic_block(mu=1,period=12,order=2,D=0.975)+
  noise_block(mu=1)+
  polynomial_block(p=1,D=0.95)*2

outcome1=Poisson(lambda='mu',data=rowSums(chickenPox[,c(2,3,5)]))
outcome2=Multinom(p=c('p.1','p.2'),data=chickenPox[,c(2,3,5)])

fitted.data=fit_model(structure,
                      Total=outcome1,
                      Proportions=outcome2)

plot(fitted.data,plot.pkg='base')
```

More applications will be presented in the advanced examples section.

# Fitting and analysing models

In this section we briefly present the usage of the `fit_model` function, along side the auxiliary functions for analyzing a fitted model, such as the `summary`, `coef`,`plot` and `forecast` methods, the `plot_lat_var`, `plot_lin_pred` and `dlm_sampling` functions. For a deep dive in the details of each argument of each function, see the documentation of those function and/or the reference manual.

## Filtering and smoothing
 
The usage of the `fit_model` function is as follows:

```{r echo=TRUE, eval=FALSE}
fit_model(
  ...,
  smooth = TRUE,
  p_monit = NA,
  c_monit = 1
)
```

The `...` argument receives `dlm_block` and `dlm_distr` objects, whose creation was described in the previous sections. In particular, if the user gives a `dlm_distr` object as a named argument, its name is used as the label for that outcome. `smooth` is a boolean indicating if the smoothed distribution of the latent variables should be evaluated (generally we recommend the users to not change this value, as the computational cost of smoothing is almost always negligible). `p_monit` and `c_monit` control the sensibility of the automated monitoring and we shall discuss their usage later on this section.

Bellow we present the code for a Poisson model:

```{r}
level <- polynomial_block(rate = 1, order = 2, D = 0.95)
season <- harmonic_block(rate = 1, period = 12, order=2, D = 0.975)

outcome <- Poisson(lambda = "rate", data = c(AirPassengers))

fitted.data <- fit_model(
  level, season,          # Strucuture
  AirPassengers = outcome # oucome
)
```


The first two lines create structural blocks representing a random walk on $\mu_t$ and a seasonal component described by harmonics. The fourth line creates a Poisson outcome such that the rate parameter `lambda` is equal to $\exp\{\text{rate}\}$, since that was the name given to the linear predictor when creating the structural blocks (for details about the linear predictor see the section about the creation of the model structure). The last line receives the model structure and the Poisson outcome and fits the model, obtaining the parameters for the filtered and smoothed distribution of all latent states.

The user can see how the model fits the data using the `plot` method, which is a wrapper for the `show_fit` function, whose syntax is as follows:

```{r echo=TRUE, eval=FALSE}
show_fit(
  model,
  pred.cred = 0.95,
  lag = 1,
  cutoff = floor(model$t/10),
  plot.pkg = "auto"
)
```

The `model` argument must be a `fitted_dlm` object (i.e., the output of the `fit_model` function). 

`pred.cred` must be a number between $0$ and $1$ representing the desired credibility of the predictions.

`lag` must be an integer representing the number of steps ahead to be used for predictions. If `lag`$<0$, the smoothed distribution is used for predictions and, if `lag`$=0$, the filtered distribution is used.

`cutoff` must be an integer representing the number of initial steps that should be skipped in the plot. Usually, the model is still learning in the initial steps, so the predictions are not reliable. The default value is one tenth of the sample size rounded down.

Lastly `plot.pkg` must be a string specifing the plot engine to be used. Should be one of `'auto'`, `'base'`, `'ggplot2'` or `'plotly'`. If `'auto'` is used, then the package will try to use `plotly`, if it fails, then it tries to use `ggplot2` and, if it also fails, then the package will use the native functions provided by `R`.


```{r, results='hide'}
# show_fit(fitted.data)
# or
plot(fitted.data, plot.pkg = "base")
```

For the sake of brevity, we will not detail the usage of the remaining functions presented in this subsection, since they have similar syntax to the `show_fit` function. We strongly advise the user to consult the reference manual of this package and the documentation of each function for detailed descriptions of any function. 

To see a summary of the fitted model, one can use the `report_dlm` function (which is wrapped in the `summary` method):


```{r}
# report_dlm(fitted.data)
# or
summary(fitted.data)
```

Notice that the coefficients shown are those of the last observation, but one can also see the summary of any particular time by changing the `t` argument:

```{r}
summary(fitted.data, t = 100)
```

For more details about the usage of the `report_dlm` function, see the associated documentation.

The user may also want to plot the latent variables, for which `plot_lat_var` function can be used:

```{r, results='hide'}
plot_lat_var(fitted.data, plot.pkg = "base")$plot
```

If the user wants to see only a restricted set of latent variables, the extra argument `var` can be used to specify the name of the variable to plot:

```{r, results='hide'}
plot_lat_var(fitted.data,'Var.Poly.Level', plot.pkg = "base")$plot
```

The user may also plot the linear predictors, for which `plot_lin_pred` function can be used:

```{r, results='hide'}
plot_lin_pred(fitted.data, plot.pkg = "base")$plot
```

Similar to the `plot_lat_var` function, the user can also specify the name of the linear predictor to be plotted by the `plot_lin_pred` function (altough, in this example, there is only one linear predictor).

Notice that, in all cases, estimation is restricted to the period where the model was fitted. If the user wishes make predictions for future observations, the `forecast` method (which is a wrapper for the `dlm_forecast` function) can be used:

```{r, results='hide'}
forecast(fitted.data, 20, plot = "base")$plot
```


For details on the usage of this function, see the associated documentation.

## Extracting components

Naturally, the user may want to extract the data from the fitted model as a matrix, array or data frame, such that more detailed analysis may be performed. For extracting the parameters of the distribution of latent states, linear predictors and observational model parameters, one can use the `coef` method, which is a wrapper for the `eval_past` function:

```{r echo=TRUE, eval=FALSE}
# eval_past(fitted.data, lag=-1)
# or
coef(fitted.data, lag = -1)
```

The output of this function is a list containing:

- ``data``: A data frame with the model evaluated at each observed time.

- ``mt``: A $n \times T$ matrix representing the mean of the latent variables at each time, where $n$ is the number of latent variables in the model and $T$ is the length of the time series;

- ``Ct``: A 3D-array containing the covariance matrix of the latent variable at each time. Dimensions are $n \times n \times T$;

- ``ft``: A $k \times T$ matrix representing the mean of the linear predictor at each time, where $k$ is the number of linear predictors in the model and $T$ is the length of the time series;

- ``Qt``: A 3D-array containing the covariance matrix for the linear predictor at each time. Dimensions are $k \times k \times T$;

- Several vectors with some metrics, including the **Mean Absolute Error** (MAE), the **Mean Absolute Scaled Error** (MASE), the **Relative Absolute Error** (RAE), the **Mean Squared Error** (MSE) and the **Interval Score** (interval.score).

- `conj.param`: A list containing the conjugated parameters for the distribution of the parameter of the observational model.

Additionally, the `dlm_forecast` function also provides a similar set of attributes, specifically, the predictive distribution for the latent states, the linear predictors and the observational model parameters, along with the predictions for future observations.

Also, each function presented in the previous subsection for visualization provides the data frame used to create the associated plot.

Lastly, altough we do not recommend it, the user may also extract some of these information directly from the `fitted_dlm` object.

We strongly recommend every user to consult the documentation of each of these functions to see the full set of features provided by the package.

## Intervention and monitoring

As an extra feature, the package has support for intervention and automated monitoring. First, if the user is aware that at some specific time there is some change in the time series that is not part of its temporal dynamic, then the user should provide that information in the model structure. For that we provide the `intervention` function:

```{r , results='hide'}
data <- c(AirPassengers)
# Adding an artificial change, so that we can make an intervention on the data at that point
# Obviously, one should NOT change their own data.
data[60:144] <- data[60:144] + 100

level <- polynomial_block(rate = 1, order = 2, D = 0.95)
season <- harmonic_block(rate = 1, order = 2, period = 12, D = 0.975)

# Reducing the discount factor so that the model can capture the expected change.
level <- level |> intervention(time = 60, H = 1, var.index = 1)
# Comment the line above to see the fit without the intervention

outcome <- Poisson(lambda = "rate", data = data)
#
fitted.data <- fit_model(level, season,
  AirPassengers = outcome
)

plot(fitted.data, plot.pkg = "base")
```

See the documentation of the `intervention` function for more details about its arguments. Also, we strongly recommend the user to consult @WestHarr-DLM, chapter 11 for a detailed discussion about Feed-Foward Interventions.

In case the user is not aware of any behavioral changes in the data, but suspects that they may have occurred at some unknown time, then we recommend the use of automated monitoring.

To fit a model using automated monitoring, the user must provide a valid value for the `p.monit` argument in the `fit_model` function. This argument receives values between $0$ and $1$, such that its value is interpreted as the prior probability (i.e., the probability before observing the data), at any given time, of behavioral change in the series that is not accommodated by the temporal dynamic.

```{r}
data <- c(AirPassengers)
# Adding an artificial change, so that we can make an intervention on the data at that point
# Obviously, one should NOT change their own data.
data[60:144] <- data[60:144] + 100

level <- polynomial_block(rate = 1, order = 2, D = 0.95)
season <- harmonic_block(rate = 1, order = 2, period = 12, D = 0.975)

outcome <- Poisson(lambda = "rate", data = data)

fitted.data <- fit_model(level, season,
  AirPassengers = outcome,
  p.monit=0.05
)

plot(fitted.data, plot.pkg = "base")
```

The approach used for automated monitoring is almost identical to that of @WestHarr-DLM, chapter 11.4, using Bayes' factor, such that `p.monit`$=0.05$ yields a threshold equivalent to that recommended in @WestHarr-DLM.

## Tools for sensibility analysis

In some situations, the user may not be sure about which value to use for some hyperparameter of the model (such as the discount factor or the order of a block) or about the inclusion of some structural block. As such, one might choose to peform a sensibility analysis on the effect of thoses choises. For such analysis, we provide the `search_model` function.

As an motivational example, let us assume that we are unsure about which value to choose for the discount factor in a polynomial trend block of a Poisson model. First, when defining the model structure, we must set the discount factor as a string, which will be used as the label for the unspecified parameter:

```{r}
level <- polynomial_block(rate = 1, order = 2, D = 'D1')
```

By setting the discount factor as a string, the structural block becomes partially **undefined**:

```{r}
summary(level)
```

As such, this block **can not** be used in the `fit_model` function:

```{r, error=TRUE}
season <- harmonic_block(rate = 1, order = 2, period = 12, D = 0.975)

outcome <- Poisson(lambda = "rate", data = c(AirPassengers))

fitted.data <- fit_model(level, season,
  AirPassengers = outcome
)
```

As the user can see in the error message above, a undefined `dlm_block` can only be used with the `search_model` function. This function is used to fit a set of models, while computing some comparative metrics.


```{r echo=TRUE, eval=FALSE}
search_model(
  ...,
  search.grid,
  condition = "TRUE",
  metric = "log.like",
  smooth = TRUE,
  lag = 1,
  pred.cred = 0.95,
  metric.cutoff = NA,
  p.monit = NA,
  c.monit = 1
)
```


The usage of the `search_model` function is almost identical to that of the `fit_model` function, the `...`, `smooth`, `p.monit` and `c.monit` arguments having the exact same meaning.

The `search.grid` argument must be a list containing, for each undefined parameter, the list of values to be tested. By default, this function will test all possible combinations of the undefined parameter. If the user wishes to skip some combinations, the `condition` argument can be used to provide a string with the criterion to determine which combination shall be evaluated.

The remaining options provide some control over the comparative metrics. The `metric` argument (`'mae'`, `'mase'`, `'rae'`, `'log.like'` or `'interval.score'`) indicates which metric to use when selecting the best model (all metrics are calculated, not matter the value of the `metric` argument, but only the best model by the chosen metric is saved). The `lag` argument indicates the number of steps ahead to be used for predictions ($0$ indicates filtered predictions and negative values indicate smoothed predictions). The `pred.cred` argument indicates the credibility of the intervals used in the Interval Score. The `metric.cutoff` argument indicates the number of initial observations to be ignored when computing the metrics.

After evaluating all valid combinations of hyper parameters, the `search_model` function returns the best model by the chosen metric, along with a data frame containing the metrics for each model evaluated.

```{r , results='hide'}
level <- polynomial_block(rate = 1, order = 2, D = 'D.level')
season <- harmonic_block(rate = 'sazo.effect', order = 2, period = 12, D = "D.sazo")

outcome <- Poisson(lambda = "rate", data = c(AirPassengers))

search_model(level, season,outcome,
  search.grid = list(
    sazo.effect = c(0, 1),
    D.level = c(seq.int(0.8, 1, l = 11)),
    D.sazo = c(seq.int(0.95, 1, l = 11))
  ),
  condition = "sazo.effect==1 | D.sazo==1"
)$search.data[1:5,] |> print()
```

It is important to note that not all hyper parameters can be tested directly by the search model function, indeed, only the components associated with $F_t$, $D_t$, $h_t$, $H_t$, $a_1$ and $R_1$ can be treated as undefined. Still, if the user wants to test some other hyper parameter that cannot be tested directly (such as the order of a polynomial block or the period of a harmonic block), he can create one block for each option and perform a sensibility analysis for the inclusion/exclusion of each block:

```{r , results='hide'}
# Creating a block for each order
level <- polynomial_block(rate = 'pol.ord.1', order = 1, D = 0.95)+
         polynomial_block(rate = 'pol.ord.2', order = 2, D = 0.95)+
         polynomial_block(rate = 'pol.ord.3', order = 3, D = 0.95)+
         polynomial_block(rate = 'pol.ord.4', order = 4, D = 0.95)
season <- harmonic_block(rate = 1, order = 2, period = 12, D = 0.975)

outcome <- Poisson(lambda = "rate", data = c(AirPassengers))

search_model(level, season,outcome,
  search.grid = list(
    # Each block can be present (1) or absent (0).
    pol.ord.1 = c(0, 1),
    pol.ord.2 = c(0, 1),
    pol.ord.3 = c(0, 1),
    pol.ord.4 = c(0, 1)
  ),
  condition = "pol.ord.1+pol.ord.2+pol.ord.3+pol.ord.4==1" # Only test combinations with exactly one polynomial block.
)$search.data[1:5,] |> print()
```

## Sampling and hyperparameter estimation

Lastly, one may also want to draw samples from the latent variables, linear predictors and/or the parameters $\eta_t$. This can be useful to evaluate non-linear functions of the model parameters or when the DGLM is only a part of a bigger model, from which the parameters are being estimated with Gibbs Algorithm. It is important to note that, with the method proposed in @ArtigokParametrico, sampling from the posterior distribution of the latent states is straight forward, allowing the user to obtain large independent (**not** approximately independent, but **exactly** independent) samples with very low computational cost. See @WestHarr-DLM, chapter 15, for details about the sampling algorithm.

Our package offers the `dlm_sampling` function, which provides a routine for drawing independent samples from any fitted model:


```{r eval=FALSE}
dlm_sampling(fitted.data, 5000)
```

For the example above, where our model has $6$ latent states and $144$ observations (which yields a total of $864$ parameters), it takes approximately $0.3$ seconds to draw a sample of size $5.000$.

Another useful feature of our package is that it provides an approximate value for the Model likelihood $f(y)= \int_{\mathbb{R^{n}}}f(y|\theta)f(\theta)d\theta$, where $y$ represents the values for $Y_t$ for all $t$ and $\theta$ represents the values the values of $\theta_t$ for all $t$. This feature can be used for two main purposes: to compare different models and to evaluate the posterior distribution of hyperparemeter. 

To compare diferent models, $\mathcal{M_1},...,\mathcal{M_k}$ , one can note that $f(\mathcal{M_i}|y) \propto f(y|\mathcal{M_i})f(\mathcal{M_i})$, where $f(y|\mathcal{M_i})= \int_{\mathbb{R^{n}}}f(y|\theta,\mathcal{M_i})f(\theta|\mathcal{M_i})d\theta$ is the likelihood of model $\mathcal{M_i}$ and $f(\mathcal{M_i})$ is the prior for model $\mathcal{M_i}$. To evaluate the $f(y|\mathcal{M_i})$, one can make use of the `eval_past` (`coef` method) or `search_model` functions by setting `lag` to a negative number, since, in that case, the log likelihood metric will be $\ln f(y|\mathcal{M_i})$. Similarly, if the user wants to obtain the marginal posterior distribution of an hyper parameter $\tau$, it can observe that $f(\tau|y) \propto f(y|\tau)f(\tau)$, from which $f(y|\tau)$ can be evaluated using the `eval_past` (`coef` method) or `search_model` functions.

```{r}
H.range=seq.int(0,1,l=100)
log.like.H=seq_along(H.range)
log.prior.H=dlnorm(H.range,0,1,log=TRUE)
for(i in seq_along(H.range)){
  level <- polynomial_block(rate = 1, order = 2, H=H.range[i])
  season <- harmonic_block(rate = 1, order = 2, period = 12, D = 0.975)
  
  # Using only 10 observations, for the sake of a pretty plot. For this particular application, the posterior density of H rapidly becomes highly consentrated in a single value.
  outcome <- Poisson(lambda = "rate", data = c(AirPassengers)[1:10]) 
  fitted.data <- fit_model(level,season,outcome)
  
  log.like.H[i]=coef(fitted.data,lag=-1)$log.like
  
}

log.post.H=log.prior.H+log.like.H

plot(H.range,exp(log.post.H-max(log.post.H)),type='l',xlab='H',ylab='f(H|y)')
```

# Advanced examples

## Dealing with overdispersion: The Noisy Poisson model <font color='red'> Em construo</font>

For this example, we will take a look in a time series consisting of monthly hospital admissions of infants (less than $5$ years old) by chicken pox in Brazil from 2008 to 2019 (see the documentation for `chickenPox`).

```{r}
plot(chickenPox$date,chickenPox$`< 5 year`,type='l',
     xlab='Year',ylab='Hospital admissions',
     main='Hospital admissions of infants by chicken pox in Brazil')
points(chickenPox$date,chickenPox$`< 5 year`,pch=0)
t=length(chickenPox$date)
vac.index=min(seq_len(t)[chickenPox$date>=as.Date('2013-09-01')])
lines(c(chickenPox$date[vac.index],chickenPox$date[vac.index]),c(-100,10000),lty=2)
```


Notice that the series above has a decaying pattern (it is **not** stationary), along with a irregular seasonal pattern (which cannot be explained by a deterministic seasonal component). Notice also that the series has very low values in recent years (duo majorly to the inclusion of a vaccine for chicken pox to the National Program of Immunization), such that a Normal model would not be appropriate for the whole series.

We will start our analysis with a very simple model and will add structure to it as we proceed with the analysis.

### Initial model: polynomial trend + harmonic trend

Let us start with a very simple model containing only a first order polynomial trend and a first order harmonic:

<div>
#### {.tabset}

```{r}
level=polynomial_block(rate=1,
                       D=0.95,
                       name='Trend')+
harmonic_block(rate=1,
               period=12,
               D=0.975,
               name='Season')

outcome=Poisson(lambda='rate',data=chickenPox$`< 5 year`)

model=fit_model(level,
                outcome)
```


##### Predictions

```{r , results='hide'}
plot(model,plot.pkg='base')

```

##### Trend

```{r , results='hide'}
plot_lat_var(model,'Trend',plot.pkg='base')$plot

```

##### Seasonality

```{r , results='hide'}
plot_lat_var(model,'Season',plot.pkg='base')$plot

```


##### Summary

```{r}
summary(model)
```

</div>


Notice that the trend seems to decayed almost constantly over time, having an almost linear pattern in several intervals. As such, it seem reasonable to use a second order polynomial with temporal dynamic for both the level and the slope:

<div>
#### {.tabset}

```{r}
level=polynomial_block(rate=1,
                       order=2,
                       D=c(0.95,0.975),
                       name='Trend')+
harmonic_block(rate=1,
               period=12,
               D=0.975,
               name='Season')

outcome=Poisson(lambda='rate',data=chickenPox$`< 5 year`)

model=fit_model(level,
                outcome)
```


##### Predictions

```{r , results='hide'}
plot(model,plot.pkg='base')

```

##### Trend

```{r , results='hide'}
plot_lat_var(model,'Trend.Level',plot.pkg='base')$plot
plot_lat_var(model,'Trend.Slope',plot.pkg='base')$plot

```

##### Seasonality

```{r , results='hide'}
plot_lat_var(model,'Season',plot.pkg='base')$plot

```


##### Summary

```{r}
summary(model)
```

</div>

Notice that we have a considerable improvement in our comparison metrics with this change, especially for the predictive log likelihood (see the summary in both fits). We leave to the reader to test higher order polynomial trends.

Is also interesting to test if higher order for the harmonic block yields a better model: 

<div>

#### {.tabset}

```{r}
level=polynomial_block(rate=1,
                       order=2,
                       D=c(0.95,0.975),
                       name='Trend')+
harmonic_block(rate=1,
               order=2,
               period=12,
               D=0.975,
               name='Season')

outcome=Poisson(lambda='rate',data=chickenPox$`< 5 year`)

model=fit_model(level,
                outcome)
```


##### Predictions

```{r , results='hide'}
plot(model,plot.pkg='base')

```

##### Trend

```{r , results='hide'}
plot_lat_var(model,'Trend.Level',plot.pkg='base')$plot
plot_lat_var(model,'Trend.Slope',plot.pkg='base')$plot

```

##### Seasonality

```{r , results='hide'}
plot_lat_var(model,'Season.Main',plot.pkg='base')$plot

```


##### Summary

```{r}
summary(model)
```

</div>

Again, we had a reasonable improvement with an higher order harmonic block. We leave to the reader to test even higher orders.




## Multivariate Normal distribuition: A bivariate stochastic volatility model <font color='red'> Em construo</font>

# References

```{r eval=FALSE, include=FALSE}
rmarkdown::render("vignettes/vignette.Rmd")
```

